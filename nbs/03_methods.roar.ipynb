{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "{}\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.roar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from cfnet.import_essentials import *\n",
    "from cfnet.interfaces import LocalCFExplanationModule\n",
    "from cfnet.datasets import TabularDataModule\n",
    "from cfnet.utils import validate_configs\n",
    "from cfnet.methods.vanilla import binary_cross_entropy\n",
    "from cfnet.training_module import grad_update, cat_normalize\n",
    "from rocourset_net.lime import LocalApprox\n",
    "from rocourset_net.training_module import l_inf_proj\n",
    "from sklearn.utils import gen_even_slices\n",
    "from sklearn.utils.validation import _num_samples\n",
    "from joblib import effective_n_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _pred_fn_approx(\n",
    "    coef: jnp.DeviceArray, \n",
    "    x: jnp.DeviceArray\n",
    ") -> jnp.DeviceArray:\n",
    "    return 1 / (1 + jnp.exp(- x @ coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _roar(\n",
    "    x: jnp.DeviceArray, # input\n",
    "    coef: jnp.DeviceArray, # lime approx weight\n",
    "    intercept: jnp.DeviceArray,\n",
    "    n_steps: int,\n",
    "    lr: float, # learning rate for each `cf` optimization step\n",
    "    lambda_: float,\n",
    "    cat_arrays: List[List[str]],\n",
    "    cat_idx: int,\n",
    "    max_delta: float,\n",
    "    n_attacker_steps: int,\n",
    "    seed: int\n",
    ") -> jnp.DeviceArray:\n",
    "    def adv_loss(delta, coef, cf, target):\n",
    "        pred = _pred_fn_approx(coef + delta, cf)\n",
    "        return jnp.mean(optax.l2_loss(pred, target))\n",
    "\n",
    "    def loss_fn_1(cf_y, y_prime):\n",
    "        return jnp.mean(binary_cross_entropy(cf_y, y_prime))\n",
    "\n",
    "    def loss_fn_2(x, cf):\n",
    "        return jnp.mean(optax.l2_loss(x, cf))\n",
    "\n",
    "    def loss_fn(cf, x, y_prime, coef):\n",
    "        cf_y = _pred_fn_approx(coef, cf)\n",
    "        return loss_fn_1(cf_y, y_prime) + lambda_ * loss_fn_2(x, cf)\n",
    "\n",
    "    def adv_step(cf, coef, target, eps: float, n_steps: int):\n",
    "        delta = jax.random.uniform(\n",
    "            key=jax.random.PRNGKey(seed), shape=coef.shape, minval=-eps, maxval=eps)\n",
    "\n",
    "        alpha = 1.25 * eps\n",
    "        g = jax.grad(adv_loss)(delta, coef, cf, target)\n",
    "        delta = delta + alpha * jnp.sign(g)\n",
    "\n",
    "        delta = l_inf_proj(delta.reshape(1, -1), eps=eps, cat_idx=cat_idx).reshape(-1)\n",
    "        return delta\n",
    "\n",
    "    @ jax.jit\n",
    "    def gen_cf_step(x, cf, opt_state: optax.OptState):\n",
    "        delta = adv_step(\n",
    "            cf, coef, y_target, \n",
    "            max_delta, \n",
    "            n_attacker_steps\n",
    "        )\n",
    "\n",
    "        g = jax.grad(loss_fn)(cf, x, y_target, coef + delta)\n",
    "        cf, opt_state = grad_update(g, cf, opt_state, opt)\n",
    "        cf = cat_normalize(cf, cat_arrays, cat_idx, hard=False)\n",
    "        return cf, opt_state\n",
    "\n",
    "    x = x.reshape(1, -1)\n",
    "    cf = jnp.array(x, copy=True)\n",
    "    \n",
    "    y_pred = _pred_fn_approx(coef, x)\n",
    "    y_target = 1. - jnp.round(y_pred)\n",
    "\n",
    "    opt = optax.rmsprop(lr)\n",
    "    opt_state = opt.init(cf)\n",
    "    for _ in tqdm(range(n_steps)):\n",
    "        cf, opt_state = gen_cf_step(x, cf, opt_state)\n",
    "\n",
    "    cf = cat_normalize(\n",
    "        cf, cat_arrays, cat_idx, hard=True)\n",
    "    return cf.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _generate_local_exp(\n",
    "    test_X: chex.ArrayBatched,\n",
    "    pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray],\n",
    "    train_X: chex.ArrayBatched,\n",
    "    cat_arrays: List[List[str]],\n",
    "    cat_idx: int,\n",
    "    n_jobs: int\n",
    "):\n",
    "    def pred_proba(x: jnp.DeviceArray):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "        prob = pred_fn(x)\n",
    "        return jnp.concatenate([1-prob, prob], axis=1)\n",
    "\n",
    "    def local_exp_step(x, coef_, intercept_, i):\n",
    "        x = x.reshape(1, -1)\n",
    "        coef, intercept = lime.extract_weights(x, pred_proba)\n",
    "        coef_[i, :] = coef\n",
    "        intercept_[i, :] = intercept\n",
    "        \n",
    "    lime = LocalApprox(\n",
    "        train_X, cat_arrays, cat_idx\n",
    "    )\n",
    "\n",
    "    coef_, intercept_ = np.empty(test_X.shape, order='F'), np.empty(test_X.shape, order='F')\n",
    "\n",
    "    if n_jobs == 1:\n",
    "        for i, x in enumerate(tqdm(test_X)):\n",
    "            local_exp_step(x, coef_, intercept_, i)\n",
    "    else:\n",
    "        # https://github.com/scikit-learn/scikit-learn/blob/36958fb240fbe435673a9e3c52e769f01f36bec0/sklearn/metrics/pairwise.py#L1568\n",
    "        Parallel(n_jobs=n_jobs, backend='threading')(\n",
    "            delayed(local_exp_step)(x, coef_, intercept_, i)\n",
    "            for i, x in enumerate(tqdm(test_X))\n",
    "        )\n",
    "    return jnp.array(coef_), jnp.array(intercept_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ROARConfig(BaseParser):\n",
    "    max_delta: float = 0.1\n",
    "    n_steps: int = 50\n",
    "    n_attacker_steps: int = 10\n",
    "    lambda_: float = 0.5\n",
    "    lr: float = 0.1\n",
    "    seed: int = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ROAR(LocalCFExplanationModule):\n",
    "    name = \"ROAR\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        configs: Union[Dict[str, Any], ROARConfig],\n",
    "        data_module: Optional[TabularDataModule] = None\n",
    "    ):\n",
    "        self.configs = validate_configs(configs, ROARConfig)\n",
    "        if data_module:\n",
    "            self.update_cat_info(data_module)\n",
    "\n",
    "    def update_cat_info(self, data_module: TabularDataModule):\n",
    "        self.X, _  = data_module.train_dataset[:]\n",
    "        return super().update_cat_info(data_module)\n",
    "\n",
    "    def generate_cf(self, \n",
    "        x: jnp.DeviceArray, # input\n",
    "        coef: jnp.DeviceArray, # lime approx weight\n",
    "        intercept: jnp.DeviceArray\n",
    "    ) -> jnp.DeviceArray:\n",
    "        return _roar(\n",
    "            x=x, # input\n",
    "            coef=coef,\n",
    "            intercept=intercept,\n",
    "            n_steps=self.configs.n_steps,\n",
    "            lr=self.configs.lr, # learning rate for each `cf` optimization step\n",
    "            lambda_=self.configs.lambda_,\n",
    "            cat_arrays=self.cat_arrays,\n",
    "            cat_idx=self.cat_idx,\n",
    "            max_delta=self.configs.max_delta,\n",
    "            n_attacker_steps=self.configs.n_attacker_steps,\n",
    "            seed=self.configs.seed\n",
    "        )\n",
    "\n",
    "    def generate_cfs(\n",
    "        self, \n",
    "        X: chex.ArrayBatched, \n",
    "        pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray], \n",
    "        is_parallel: bool = False\n",
    "    ) -> chex.ArrayBatched:\n",
    "        print('generating local explanations via lime...')\n",
    "        coef, intercept = _generate_local_exp(\n",
    "            test_X=X, pred_fn=pred_fn, train_X=self.X,\n",
    "            cat_arrays=self.cat_arrays, cat_idx=self.cat_idx, n_jobs=-1)\n",
    "\n",
    "        def _generate_cf(x: jnp.DeviceArray, coef, intercept) -> jnp.ndarray:\n",
    "            return self.generate_cf(x, coef, intercept)\n",
    "        return jax.vmap(_generate_cf)(X, coef, intercept) if not is_parallel \\\n",
    "            else jax.pmap(_generate_cf)(X, coef, intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated\n",
    "class ROAR(LocalCFExplanationModule):\n",
    "    name = \"ROAR\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        configs: Union[Dict[str, Any], ROARConfig],\n",
    "        data_module: Optional[TabularDataModule] = None\n",
    "    ):\n",
    "        self.configs = validate_configs(configs, ROARConfig)\n",
    "        if data_module:\n",
    "            self.update_cat_info(data_module)\n",
    "\n",
    "    def update_cat_info(self, data_module: TabularDataModule):\n",
    "        self.X, _  = data_module.train_dataset[:]\n",
    "        return super().update_cat_info(data_module)\n",
    "\n",
    "    def pred_fn_approx(self, coef, x):\n",
    "        return 1 / (1 + jnp.exp(- x @ coef))\n",
    "\n",
    "    def generate_cf(self, \n",
    "        x: jnp.DeviceArray, # input\n",
    "        coef: jnp.DeviceArray, # lime approx weight\n",
    "        intercept: jnp.DeviceArray\n",
    "    ) -> jnp.DeviceArray:\n",
    "        def adv_loss(delta, coef, cf, target):\n",
    "            pred = self.pred_fn_approx(coef + delta, cf)\n",
    "            return jnp.mean(optax.l2_loss(pred, target))\n",
    "\n",
    "        def loss_fn_1(cf_y, y_prime):\n",
    "            # return jnp.mean(optax.l2_loss(cf_y, y_prime))\n",
    "            return jnp.mean(binary_cross_entropy(cf_y, y_prime))\n",
    "\n",
    "        def loss_fn_2(x, cf):\n",
    "            return jnp.mean(optax.l2_loss(x, cf))\n",
    "\n",
    "        def loss_fn(cf, x, y_prime, coef):\n",
    "            cf_y = self.pred_fn_approx(coef, cf)\n",
    "            return loss_fn_1(cf_y, y_prime) + self.configs.lambda_ * loss_fn_2(x, cf)\n",
    "\n",
    "        def adv_step(cf, coef, target, eps: float, n_steps: int):\n",
    "            delta = jax.random.uniform(\n",
    "                key=next(self.configs.keys), \n",
    "                shape=coef.shape, minval=-eps, maxval=eps)\n",
    "\n",
    "            alpha = 1.25 * eps\n",
    "            g = jax.grad(adv_loss)(delta, coef, cf, target)\n",
    "            delta = delta + alpha * jnp.sign(g)\n",
    "\n",
    "            delta = l_inf_proj(delta.reshape(1, -1), eps=eps, cat_idx=self.cat_idx).reshape(-1)\n",
    "            return delta\n",
    "\n",
    "        @ jax.jit\n",
    "        def gen_cf_step(x, cf, opt_state: optax.OptState):\n",
    "            delta = adv_step(\n",
    "                cf, coef, y_target, \n",
    "                self.configs.max_delta, \n",
    "                self.configs.n_attacker_steps\n",
    "            )\n",
    "\n",
    "            g = jax.grad(loss_fn)(cf, x, y_target, coef + delta)\n",
    "            cf, opt_state = grad_update(g, cf, opt_state, opt)\n",
    "            cf = cat_normalize(cf, self.cat_arrays, self.cat_idx, hard=False)\n",
    "            return cf, opt_state\n",
    "\n",
    "        x = x.reshape(1, -1)\n",
    "        cf = jnp.array(x, copy=True)\n",
    "        \n",
    "        y_pred = self.pred_fn_approx(coef, x)\n",
    "        y_target = 1. - jnp.round(y_pred)\n",
    "\n",
    "        opt = optax.rmsprop(self.configs.lr)\n",
    "        opt_state = opt.init(cf)\n",
    "        for _ in tqdm(range(self.configs.n_steps)):\n",
    "            cf, opt_state = gen_cf_step(x, cf, opt_state)\n",
    "\n",
    "        cf = cat_normalize(\n",
    "            cf, self.cat_arrays, self.cat_idx, hard=True)\n",
    "        return cf.reshape(-1)\n",
    "        \n",
    "    def generate_local_exps(\n",
    "        self,\n",
    "        X: chex.ArrayBatched, \n",
    "        pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray]\n",
    "    ):\n",
    "        def _pred_fn(x: jnp.DeviceArray):\n",
    "            if len(x.shape) == 1:\n",
    "                x = x.reshape(1, -1)\n",
    "            prob = pred_fn(x)\n",
    "            return jnp.concatenate([1-prob, prob], axis=1)\n",
    "\n",
    "        lime = LocalApprox(self.X, self.cat_arrays, self.cat_idx)\n",
    "        coef_, intercept_ = [], []\n",
    "        for x in tqdm(X): # TODO: optimize for parallel computing\n",
    "            x = x.reshape(1, -1)\n",
    "            coef, intercept = lime.extract_weights(x, _pred_fn)\n",
    "            # assert jnp.abs(self.pred_fn_approx(coef, x) - \\\n",
    "            #     pred_fn(x)) < 0.05, \\\n",
    "            #     f\"self.pred_fn_approx(coef, x)={self.pred_fn_approx(coef, x)}\" + \\\n",
    "            #         f\"pred_fn(x)={pred_fn(x)}\"\n",
    "            coef_.append(coef); intercept_.append(intercept)\n",
    "        return np.array(coef_), np.array(intercept_)\n",
    "    \n",
    "    def generate_cfs(\n",
    "        self, \n",
    "        X: chex.ArrayBatched, \n",
    "        pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray], \n",
    "        is_parallel: bool = False\n",
    "    ) -> chex.ArrayBatched:\n",
    "        print('generating local explanations via lime...')\n",
    "        coef, intercept = self.generate_local_exps(X, pred_fn)\n",
    "        def _generate_cf(x: jnp.DeviceArray, coef, intercept) -> jnp.ndarray:\n",
    "            return self.generate_cf(x, coef, intercept)\n",
    "        return jax.vmap(_generate_cf)(X, coef, intercept) if not is_parallel \\\n",
    "            else jax.pmap(_generate_cf)(X, coef, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 3/3 [00:00<00:00, 85.25batch/s, train/train_loss_1=0.0329]\n"
     ]
    }
   ],
   "source": [
    "from cfnet.train import train_model\n",
    "from cfnet.training_module import PredictiveTrainingModule\n",
    "from cfnet.evaluate import generate_cf_results_local_exp\n",
    "\n",
    "\n",
    "dm = TabularDataModule({\n",
    "    \"data_name\": \"student\",\n",
    "    \"continous_cols\": [\n",
    "        \"failures\", \"age\"\n",
    "    ],\n",
    "    \"discret_cols\": [\n",
    "        \"G2\", \"G1\", \"higher\", \"goout\", \"Mjob\", \"Fjob\", \"health\", \n",
    "        \"freetime\", \"absences\", \"Walc\", \"famrel\", \"Medu\", \"Fedu\"\n",
    "    ],\n",
    "    \"batch_size\": 128,\n",
    "    \"data_dir\": \"assets/data/student/gp.csv\"\n",
    "})\n",
    "m_configs = {\n",
    "    'lr': 0.03,\n",
    "    \"sizes\": [50, 10, 50],\n",
    "    \"dropout_rate\": 0.3\n",
    "}\n",
    "t_configs = {\n",
    "    'n_epochs': 10,\n",
    "    'monitor_metrics': 'val/val_loss',\n",
    "    'logger_name': 'pred'\n",
    "}\n",
    "\n",
    "dm.prepare_data()\n",
    "\n",
    "training_module = PredictiveTrainingModule(m_configs)\n",
    "param, _ = train_model(\n",
    "    training_module, dm, t_configs=t_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "_param = deepcopy(param)\n",
    "pred_fn = lambda x: training_module.forward(_param, random.PRNGKey(0), x, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.9339623, dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test prediction\n",
    "X, y = dm.test_dataset[:]\n",
    "jnp.mean(jnp.round(pred_fn(X)) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating local explanations via lime...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:02<00:00, 47.87it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 57.15it/s]\n"
     ]
    }
   ],
   "source": [
    "cf_exp = ROAR({'lr': 0.01, 'n_steps': 100, 'lambda_': 0.5})\n",
    "res = generate_cf_results_local_exp(cf_exp, dm, pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ROAR': {'acc': 0.9339622855186462,\n",
       "  'validity': 0.9811320900917053,\n",
       "  'proximity': 17.964111328125}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cfnet.evaluate import evaluate_cfs\n",
    "\n",
    "evaluate_cfs(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:05<00:00, 20.90it/s]\n",
      "100%|██████████| 106/106 [00:04<00:00, 22.24it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 106 is different from 82)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5510/2557895212.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# print(_pred_fn_approx(w, b, x),pred_fn(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcf_exp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_fn_approx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5510/195704274.py\u001b[0m in \u001b[0;36mpred_fn_approx\u001b[0;34m(self, coef, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpred_fn_approx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     def generate_cf(self, \n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 106 is different from 82)"
     ]
    }
   ],
   "source": [
    "# test lime fidelity\n",
    "\n",
    "def pred_proba(x: jnp.DeviceArray):\n",
    "    if len(x.shape) == 1:\n",
    "        x = x.reshape(1, -1)\n",
    "    prob = pred_fn(x)\n",
    "    return jnp.concatenate([1-prob, prob], axis=1)\n",
    "\n",
    "test_X, _ = dm.test_dataset[:]\n",
    "train_X, _ = dm.train_dataset[:]\n",
    "\n",
    "coef, intercept = _generate_local_exp(\n",
    "    test_X=test_X,\n",
    "    pred_fn=pred_fn,\n",
    "    train_X=train_X,\n",
    "    cat_arrays=dm.cat_arrays,\n",
    "    cat_idx=dm.cat_idx,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "cf_exp = ROAR({'lr': 0.01, 'n_steps': 100, 'lambda_': 0.5})\n",
    "cf_exp.update_cat_info(dm)\n",
    "coef, _ = cf_exp.generate_local_exps(test_X, pred_fn)\n",
    "\n",
    "for x, w, b in zip(test_X, coef, intercept): \n",
    "    x = x.reshape(1, -1)\n",
    "    # print(_pred_fn_approx(w, b, x),pred_fn(x))\n",
    "    assert np.abs(cf_exp.pred_fn_approx(coef, x) - pred_fn(x)).item() < 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-2.35665   ,  1.5335364 ,  3.3647377 , ..., -0.45213205,\n",
       "              -0.6246655 , -0.5832944 ],\n",
       "             [-2.2370493 ,  1.5584421 ,  3.403727  , ..., -0.51030767,\n",
       "              -0.61729926, -0.6108913 ],\n",
       "             [-1.6378618 ,  1.4759126 ,  3.2155807 , ..., -0.50645286,\n",
       "              -0.65963054, -0.56697714],\n",
       "             ...,\n",
       "             [-1.4451138 ,  1.574613  ,  3.036847  , ..., -0.6054619 ,\n",
       "              -0.773767  , -0.575     ],\n",
       "             [-2.1144795 ,  1.4843261 ,  3.2143805 , ..., -0.474191  ,\n",
       "              -0.68732035, -0.5306021 ],\n",
       "             [-1.2243404 ,  1.7315732 ,  3.0439065 , ..., -0.5330738 ,\n",
       "              -0.65676194, -0.62791127]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
