{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.roar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from relax.import_essentials import *\n",
    "from relax.data import TabularDataModule\n",
    "from relax.utils import validate_configs\n",
    "from relax.methods.vanilla import binary_cross_entropy\n",
    "from relax.methods.base import BaseCFModule, BaseParametricCFModule, BasePredFnCFModule\n",
    "from relax.utils import binary_cross_entropy, grad_update, cat_normalize\n",
    "from rocourse_net.lime import LocalApprox\n",
    "from rocourse_net.module import l_inf_proj\n",
    "from sklearn.utils import gen_even_slices\n",
    "from sklearn.utils.validation import _num_samples\n",
    "from joblib import effective_n_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _pred_fn_approx(\n",
    "    coef: jnp.DeviceArray, \n",
    "    x: jnp.DeviceArray\n",
    ") -> jnp.DeviceArray:\n",
    "    return 1 / (1 + jnp.exp(- x @ coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _roar(\n",
    "    x: jnp.DeviceArray, # input\n",
    "    coef: jnp.DeviceArray, # lime approx weight\n",
    "    intercept: jnp.DeviceArray,\n",
    "    n_steps: int,\n",
    "    lr: float, # learning rate for each `cf` optimization step\n",
    "    lambda_: float,\n",
    "    cat_arrays: List[List[str]],\n",
    "    cat_idx: int,\n",
    "    max_delta: float,\n",
    "    n_attacker_steps: int,\n",
    "    seed: int\n",
    ") -> jnp.DeviceArray:\n",
    "    def adv_loss(delta, coef, cf, target):\n",
    "        pred = _pred_fn_approx(coef + delta, cf)\n",
    "        return jnp.mean(optax.l2_loss(pred, target))\n",
    "\n",
    "    def loss_fn_1(cf_y, y_prime):\n",
    "        return jnp.mean(binary_cross_entropy(cf_y, y_prime))\n",
    "\n",
    "    def loss_fn_2(x, cf):\n",
    "        return jnp.mean(optax.l2_loss(x, cf))\n",
    "\n",
    "    def loss_fn(cf, x, y_prime, coef):\n",
    "        cf_y = _pred_fn_approx(coef, cf)\n",
    "        return loss_fn_1(cf_y, y_prime) + lambda_ * loss_fn_2(x, cf)\n",
    "\n",
    "    def adv_step(cf, coef, target, eps: float, n_steps: int):\n",
    "        delta = jax.random.uniform(\n",
    "            key=jax.random.PRNGKey(seed), shape=coef.shape, minval=-eps, maxval=eps)\n",
    "\n",
    "        alpha = 1.25 * eps\n",
    "        g = jax.grad(adv_loss)(delta, coef, cf, target)\n",
    "        delta = delta + alpha * jnp.sign(g)\n",
    "\n",
    "        delta = l_inf_proj(delta.reshape(1, -1), eps=eps, cat_idx=cat_idx).reshape(-1)\n",
    "        return delta\n",
    "\n",
    "    @ jax.jit\n",
    "    def gen_cf_step(x, cf, opt_state: optax.OptState):\n",
    "        delta = adv_step(\n",
    "            cf, coef, y_target, \n",
    "            max_delta, \n",
    "            n_attacker_steps\n",
    "        )\n",
    "\n",
    "        g = jax.grad(loss_fn)(cf, x, y_target, coef + delta)\n",
    "        cf, opt_state = grad_update(g, cf, opt_state, opt)\n",
    "        cf = cat_normalize(cf, cat_arrays, cat_idx, hard=False)\n",
    "        return cf, opt_state\n",
    "\n",
    "    x = x.reshape(1, -1)\n",
    "    cf = jnp.array(x, copy=True)\n",
    "    \n",
    "    y_pred = _pred_fn_approx(coef, x)\n",
    "    y_target = 1. - jnp.round(y_pred)\n",
    "\n",
    "    opt = optax.rmsprop(lr)\n",
    "    opt_state = opt.init(cf)\n",
    "    for _ in tqdm(range(n_steps)):\n",
    "        cf, opt_state = gen_cf_step(x, cf, opt_state)\n",
    "\n",
    "    cf = cat_normalize(\n",
    "        cf, cat_arrays, cat_idx, hard=True)\n",
    "    return cf.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _generate_local_exp(\n",
    "    test_X: chex.ArrayBatched,\n",
    "    pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray],\n",
    "    train_X: chex.ArrayBatched,\n",
    "    cat_arrays: List[List[str]],\n",
    "    cat_idx: int,\n",
    "    n_jobs: int\n",
    "):\n",
    "    def pred_proba(x: jnp.DeviceArray):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "        prob = pred_fn(x)\n",
    "        return jnp.concatenate([1-prob, prob], axis=1)\n",
    "\n",
    "    def local_exp_step(x, coef_, intercept_, i):\n",
    "        x = x.reshape(1, -1)\n",
    "        coef, intercept = lime.extract_weights(x, pred_proba)\n",
    "        coef_[i, :] = coef\n",
    "        intercept_[i, :] = intercept\n",
    "        \n",
    "    lime = LocalApprox(\n",
    "        train_X, cat_arrays, cat_idx\n",
    "    )\n",
    "\n",
    "    coef_, intercept_ = np.empty(test_X.shape, order='F'), np.empty(test_X.shape, order='F')\n",
    "\n",
    "    if n_jobs == 1:\n",
    "        for i, x in enumerate(tqdm(test_X)):\n",
    "            local_exp_step(x, coef_, intercept_, i)\n",
    "    else:\n",
    "        # https://github.com/scikit-learn/scikit-learn/blob/36958fb240fbe435673a9e3c52e769f01f36bec0/sklearn/metrics/pairwise.py#L1568\n",
    "        Parallel(n_jobs=n_jobs, backend='threading')(\n",
    "            delayed(local_exp_step)(x, coef_, intercept_, i)\n",
    "            for i, x in enumerate(tqdm(test_X))\n",
    "        )\n",
    "    return jnp.array(coef_), jnp.array(intercept_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ROARConfig(BaseParser):\n",
    "    max_delta: float = 0.1\n",
    "    n_steps: int = 50\n",
    "    n_attacker_steps: int = 10\n",
    "    lambda_: float = 0.5\n",
    "    lr: float = 0.1\n",
    "    seed: int = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ROAR(BaseCFModule):\n",
    "    name = \"ROAR\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        configs: Union[Dict[str, Any], ROARConfig],\n",
    "        data_module: Optional[TabularDataModule] = None\n",
    "    ):\n",
    "        self.configs = validate_configs(configs, ROARConfig)\n",
    "        if data_module:\n",
    "            self.update_cat_info(data_module)\n",
    "\n",
    "    def update_cat_info(self, data_module: TabularDataModule):\n",
    "        self.X, _  = data_module.train_dataset[:]\n",
    "        return super().update_cat_info(data_module)\n",
    "\n",
    "    def generate_cf(self, \n",
    "        x: jnp.DeviceArray, # input\n",
    "        coef: jnp.DeviceArray, # lime approx weight\n",
    "        intercept: jnp.DeviceArray\n",
    "    ) -> jnp.DeviceArray:\n",
    "        return _roar(\n",
    "            x=x, # input\n",
    "            coef=coef,\n",
    "            intercept=intercept,\n",
    "            n_steps=self.configs.n_steps,\n",
    "            lr=self.configs.lr, # learning rate for each `cf` optimization step\n",
    "            lambda_=self.configs.lambda_,\n",
    "            cat_arrays=self.cat_arrays,\n",
    "            cat_idx=self.cat_idx,\n",
    "            max_delta=self.configs.max_delta,\n",
    "            n_attacker_steps=self.configs.n_attacker_steps,\n",
    "            seed=self.configs.seed\n",
    "        )\n",
    "\n",
    "    def generate_cfs(\n",
    "        self, \n",
    "        X: chex.ArrayBatched, \n",
    "        pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray], \n",
    "        is_parallel: bool = False\n",
    "    ) -> chex.ArrayBatched:\n",
    "        print('generating local explanations via lime...')\n",
    "        X, _ = self.data_module.train_dataset[:]\n",
    "        self.cat_arrays = self.data_module.cat_encoder.categories_ \\\n",
    "            if self.data_module._configs.discret_cols else []\n",
    "        self.cat_idx = self.data_module.cat_idx\n",
    "        coef, intercept = _generate_local_exp(\n",
    "            test_X=X, pred_fn=pred_fn, train_X=X,\n",
    "            cat_arrays=self.cat_arrays, \n",
    "            cat_idx=self.cat_idx, n_jobs=-1\n",
    "        )\n",
    "\n",
    "        def _generate_cf(x: jnp.DeviceArray, coef, intercept) -> jnp.ndarray:\n",
    "            return self.generate_cf(x, coef, intercept)\n",
    "        return jax.vmap(_generate_cf)(X, coef, intercept) if not is_parallel \\\n",
    "            else jax.pmap(_generate_cf)(X, coef, intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated\n",
    "# class ROAR(BaseCFModule):\n",
    "    name = \"ROAR\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        configs: Union[Dict[str, Any], ROARConfig],\n",
    "        data_module: Optional[TabularDataModule] = None\n",
    "    ):\n",
    "        self.configs = validate_configs(configs, ROARConfig)\n",
    "        if data_module:\n",
    "            self.update_cat_info(data_module)\n",
    "\n",
    "    def update_cat_info(self, data_module: TabularDataModule):\n",
    "        self.X, _  = data_module.train_dataset[:]\n",
    "        return super().update_cat_info(data_module)\n",
    "\n",
    "    def pred_fn_approx(self, coef, x):\n",
    "        return 1 / (1 + jnp.exp(- x @ coef))\n",
    "\n",
    "    def generate_cf(self, \n",
    "        x: jnp.DeviceArray, # input\n",
    "        coef: jnp.DeviceArray, # lime approx weight\n",
    "        intercept: jnp.DeviceArray\n",
    "    ) -> jnp.DeviceArray:\n",
    "        def adv_loss(delta, coef, cf, target):\n",
    "            pred = self.pred_fn_approx(coef + delta, cf)\n",
    "            return jnp.mean(optax.l2_loss(pred, target))\n",
    "\n",
    "        def loss_fn_1(cf_y, y_prime):\n",
    "            # return jnp.mean(optax.l2_loss(cf_y, y_prime))\n",
    "            return jnp.mean(binary_cross_entropy(cf_y, y_prime))\n",
    "\n",
    "        def loss_fn_2(x, cf):\n",
    "            return jnp.mean(optax.l2_loss(x, cf))\n",
    "\n",
    "        def loss_fn(cf, x, y_prime, coef):\n",
    "            cf_y = self.pred_fn_approx(coef, cf)\n",
    "            return loss_fn_1(cf_y, y_prime) + self.configs.lambda_ * loss_fn_2(x, cf)\n",
    "\n",
    "        def adv_step(cf, coef, target, eps: float, n_steps: int):\n",
    "            delta = jax.random.uniform(\n",
    "                key=next(self.configs.keys), \n",
    "                shape=coef.shape, minval=-eps, maxval=eps)\n",
    "\n",
    "            alpha = 1.25 * eps\n",
    "            g = jax.grad(adv_loss)(delta, coef, cf, target)\n",
    "            delta = delta + alpha * jnp.sign(g)\n",
    "\n",
    "            delta = l_inf_proj(delta.reshape(1, -1), eps=eps, cat_idx=self.cat_idx).reshape(-1)\n",
    "            return delta\n",
    "\n",
    "        @ jax.jit\n",
    "        def gen_cf_step(x, cf, opt_state: optax.OptState):\n",
    "            delta = adv_step(\n",
    "                cf, coef, y_target, \n",
    "                self.configs.max_delta, \n",
    "                self.configs.n_attacker_steps\n",
    "            )\n",
    "\n",
    "            g = jax.grad(loss_fn)(cf, x, y_target, coef + delta)\n",
    "            cf, opt_state = grad_update(g, cf, opt_state, opt)\n",
    "            cf = cat_normalize(cf, self.cat_arrays, self.cat_idx, hard=False)\n",
    "            return cf, opt_state\n",
    "\n",
    "        x = x.reshape(1, -1)\n",
    "        cf = jnp.array(x, copy=True)\n",
    "        \n",
    "        y_pred = self.pred_fn_approx(coef, x)\n",
    "        y_target = 1. - jnp.round(y_pred)\n",
    "\n",
    "        opt = optax.rmsprop(self.configs.lr)\n",
    "        opt_state = opt.init(cf)\n",
    "        for _ in tqdm(range(self.configs.n_steps)):\n",
    "            cf, opt_state = gen_cf_step(x, cf, opt_state)\n",
    "\n",
    "        cf = cat_normalize(\n",
    "            cf, self.cat_arrays, self.cat_idx, hard=True)\n",
    "        return cf.reshape(-1)\n",
    "        \n",
    "    def generate_local_exps(\n",
    "        self,\n",
    "        X: chex.ArrayBatched, \n",
    "        pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray]\n",
    "    ):\n",
    "        def _pred_fn(x: jnp.DeviceArray):\n",
    "            if len(x.shape) == 1:\n",
    "                x = x.reshape(1, -1)\n",
    "            prob = pred_fn(x)\n",
    "            return jnp.concatenate([1-prob, prob], axis=1)\n",
    "\n",
    "        lime = LocalApprox(self.X, self.cat_arrays, self.cat_idx)\n",
    "        coef_, intercept_ = [], []\n",
    "        for x in tqdm(X): # TODO: optimize for parallel computing\n",
    "            x = x.reshape(1, -1)\n",
    "            coef, intercept = lime.extract_weights(x, _pred_fn)\n",
    "            # assert jnp.abs(self.pred_fn_approx(coef, x) - \\\n",
    "            #     pred_fn(x)) < 0.05, \\\n",
    "            #     f\"self.pred_fn_approx(coef, x)={self.pred_fn_approx(coef, x)}\" + \\\n",
    "            #         f\"pred_fn(x)={pred_fn(x)}\"\n",
    "            coef_.append(coef); intercept_.append(intercept)\n",
    "        return np.array(coef_), np.array(intercept_)\n",
    "    \n",
    "    def generate_cfs(\n",
    "        self, \n",
    "        X: chex.ArrayBatched, \n",
    "        pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray], \n",
    "        is_parallel: bool = False\n",
    "    ) -> chex.ArrayBatched:\n",
    "        print('generating local explanations via lime...')\n",
    "        coef, intercept = self.generate_local_exps(X, pred_fn)\n",
    "        def _generate_cf(x: jnp.DeviceArray, coef, intercept) -> jnp.ndarray:\n",
    "            return self.generate_cf(x, coef, intercept)\n",
    "        return jax.vmap(_generate_cf)(X, coef, intercept) if not is_parallel \\\n",
    "            else jax.pmap(_generate_cf)(X, coef, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 3/3 [00:00<00:00, 100.39batch/s, train/train_loss_1=0.0374]\n"
     ]
    }
   ],
   "source": [
    "from relax.trainer import train_model\n",
    "from relax.module import PredictiveTrainingModule\n",
    "from relax.evaluate import generate_cf_explanations\n",
    "\n",
    "\n",
    "dm = TabularDataModule({\n",
    "    \"data_name\": \"student\",\n",
    "    \"continous_cols\": [\n",
    "        \"failures\", \"age\"\n",
    "    ],\n",
    "    \"discret_cols\": [\n",
    "        \"G2\", \"G1\", \"higher\", \"goout\", \"Mjob\", \"Fjob\", \"health\", \n",
    "        \"freetime\", \"absences\", \"Walc\", \"famrel\", \"Medu\", \"Fedu\"\n",
    "    ],\n",
    "    \"data_dir\": \"assets/data/student/gp.csv\"\n",
    "})\n",
    "m_configs = {\n",
    "    'lr': 0.03,\n",
    "    \"sizes\": [50, 10, 50],\n",
    "    \"dropout_rate\": 0.3,\n",
    "}\n",
    "t_configs = {\n",
    "    'n_epochs': 10,\n",
    "    'monitor_metrics': 'val/val_loss',\n",
    "    'logger_name': 'pred',\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "\n",
    "dm.prepare_data()\n",
    "\n",
    "training_module = PredictiveTrainingModule(m_configs)\n",
    "param, _ = train_model(\n",
    "    training_module, dm, t_configs=t_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "_param = deepcopy(param)\n",
    "pred_fn = lambda x: training_module.forward(_param, random.PRNGKey(0), x, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.8490566, dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test prediction\n",
    "X, y = dm.test_dataset[:]\n",
    "jnp.mean(jnp.round(pred_fn(X)) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mgenerate_cf_explanations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcf_module\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'BaseCFModule'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdatamodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'TabularDataModule'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpred_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'callable'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mt_configs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'TrainingConfigs'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpred_fn_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dict'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'Explanation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Generate CF explanations.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/code/ReLax/relax/evaluate.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "?generate_cf_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating local explanations via lime...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 317/317 [00:08<00:00, 36.28it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 64.51it/s]\n"
     ]
    }
   ],
   "source": [
    "cf_exp = ROAR({'lr': 0.01, 'n_steps': 100, 'lambda_': 0.5})\n",
    "res = generate_cf_explanations(\n",
    "    cf_exp, dm, pred_fn,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev2",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
