{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.counternet import (\n",
    "    CounterNetTrainingModule, \n",
    "    CounterNetTrainingModuleConfigs, \n",
    "    partition_trainable_params,\n",
    "    CounterNet,\n",
    "    CounterNetConfigs\n",
    ")\n",
    "from relax.data import load_data\n",
    "from relax.utils import grad_update\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "import chex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def l_inf_proj(x: jnp.ndarray, eps: float, cat_idx: Optional[int] = None):\n",
    "    if cat_idx is None:\n",
    "        return x.clip(-eps, eps)\n",
    "    else:\n",
    "        return jnp.concatenate([\n",
    "            x[:, :cat_idx].clip(-eps, eps), # clip continuous features only\n",
    "            x[:, cat_idx:]\n",
    "        ], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def filter_params(params):\n",
    "    return hk.data_structures.filter(\n",
    "        lambda m, n, v: m == 'counter_net_model/Predictor/dense_block/linear' and n == 'w', params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Attacker(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: hk.PRNGSequence, # ignored\n",
    "        pred_loss_fn: Callable[[hk.Params, random.PRNGKey, Tuple[jnp.DeviceArray, jnp.DeviceArray], bool], jnp.DeviceArray],\n",
    "        adv_loss_fn, # ignored \n",
    "        n_steps: int, # attacker steps\n",
    "        k: int, # inner steps\n",
    "        epsilon: float,\n",
    "        adv_lr: float,\n",
    "        apply_fn: Callable, # apply_fn(x, cf, hard=False)\n",
    "        cat_idx, # ignored\n",
    "        check_assertions: bool = False,\n",
    "    ):\n",
    "        self.keys = keys\n",
    "        self.pred_loss_fn = pred_loss_fn\n",
    "        self.adv_loss_fn = adv_loss_fn\n",
    "        self.n_steps = n_steps\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.adv_lr = adv_lr\n",
    "        self.apply_fn = apply_fn\n",
    "        self.cat_idx = cat_idx\n",
    "        self.check_assertions = check_assertions\n",
    "    \n",
    "    def step(\n",
    "        self,\n",
    "        params: hk.Params,\n",
    "        x: jnp.ndarray, \n",
    "        y: jnp.ndarray,\n",
    "    ) -> hk.Params:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    __call__ = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "key, *subkeys = random.split(key, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RandomAttacker(Attacker):\n",
    "    def step(self, params, rng_key, x, y):\n",
    "        # init delta randomly\n",
    "        delta = random.uniform(\n",
    "            key=rng_key, shape=x.shape, \n",
    "            minval=-self.epsilon, maxval=self.epsilon)\n",
    "        \n",
    "        # create optimizer\n",
    "        opt = optax.adam(learning_rate=self.adv_lr)\n",
    "        opt_state = opt.init(params)\n",
    "\n",
    "        for _ in range(self.n_steps):\n",
    "            rng_key, *subkeys = random.split(rng_key, self.k + 2)\n",
    "            x = self.apply_fn(x, x + delta, hard=False)\n",
    "\n",
    "            for i in range(self.k):\n",
    "                loss, grads = jax.value_and_grad(self.pred_loss_fn)(\n",
    "                        params, subkeys[i], (x, y), False)\n",
    "                params, opt_state = grad_update(grads, params, opt_state, opt)\n",
    "\n",
    "            delta = random.uniform(\n",
    "                key=subkeys[-1], shape=x.shape, \n",
    "                minval=-self.epsilon, maxval=self.epsilon)\n",
    "            rng_key = subkeys[-1]\n",
    "            \n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BilevelAttacker(Attacker):\n",
    "    def step(self, params: hk.Params, rng_key: random.PRNGKey, x: jnp.ndarray, y: jnp.ndarray) -> hk.Params:\n",
    "        # alpha is the delta's learning rate\n",
    "        alpha = self.epsilon * 2.5 / self.n_steps\n",
    "        # init delta randomly\n",
    "        delta = random.uniform(\n",
    "            key=rng_key, shape=x.shape, \n",
    "            minval=-self.epsilon, maxval=self.epsilon)\n",
    "        # create optimizer\n",
    "        opt = optax.chain(\n",
    "            optax.clip(1.0),\n",
    "            optax.adam(learning_rate=self.adv_lr)\n",
    "        )\n",
    "        \n",
    "        opt_state = opt.init(params)\n",
    "\n",
    "        @partial(jax.jit, static_argnames=['opt'])\n",
    "        def attacker_fn(\n",
    "            delta: jnp.ndarray,\n",
    "            params: hk.Params,\n",
    "            opt_state: optax.OptState,\n",
    "            rng_key: random.PRNGKey,\n",
    "            batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
    "            opt: optax.GradientTransformation\n",
    "        ):\n",
    "            # def inner_step(states, k):\n",
    "            #     params, opt_state, rng_key = states\n",
    "            #     rng_key, sub_key = random.split(rng_key)\n",
    "            #     _x = self.apply_fn(x, x + delta, hard=False)\n",
    "            #     grads = jax.grad(self.pred_loss_fn)(params, rng_key, (_x, y), is_training=True)\n",
    "            #     params, opt_state = grad_update(grads, params, opt_state, opt)\n",
    "            #     return (params, opt_state, sub_key), None\n",
    "\n",
    "            # x, y = batch\n",
    "            # states = (params, opt_state, rng_key)\n",
    "            # (params, opt_state, rng_key), _ = jax.lax.scan(inner_step, states, jnp.arange(self.k))\n",
    "            # loss = self.adv_loss_fn(params, rng_key, x, is_training=False)\n",
    "            # return loss, (params, opt_state)\n",
    "\n",
    "            x, y = batch\n",
    "            for _ in range(self.k):\n",
    "                # inner unrolling steps\n",
    "                _x = self.apply_fn(x, x + delta, hard=False)\n",
    "                grads = jax.grad(self.pred_loss_fn)(params, rng_key, (_x, y), is_training=False)\n",
    "                params, opt_state = grad_update(grads, params, opt_state, opt)\n",
    "\n",
    "            loss = self.adv_loss_fn(params, rng_key, x, is_training=False)\n",
    "            return loss, (params, opt_state)\n",
    "\n",
    "        def attacker_step(states, k):\n",
    "            delta, params, opt_state, rng_key = states\n",
    "            rng_key, sub_key = random.split(rng_key)\n",
    "            g, (params, opt_state) = jax.grad(attacker_fn, has_aux=True)(\n",
    "                delta, params, opt_state, rng_key, (x, y), opt)\n",
    "\n",
    "            g = jnp.clip(g, -1.0, 1.0)\n",
    "            delta = delta + alpha * jnp.sign(g)\n",
    "            delta = l_inf_proj(delta, self.epsilon, self.cat_idx)\n",
    "            return (delta, params, opt_state, sub_key), None\n",
    "\n",
    "        states = (delta, params, opt_state, rng_key)\n",
    "        (delta, params, opt_state, rng_key), _ = jax.lax.scan(\n",
    "            f=attacker_step, init=states, xs=jnp.arange(self.n_steps))\n",
    "\n",
    "        # for i in range(self.n_steps):\n",
    "        #     _, rng_key = random.split(rng_key)\n",
    "            \n",
    "            # g, (params, opt_state) = jax.grad(attacker_fn, has_aux=True)(\n",
    "            #     delta, params, opt_state, rng_key, (x, y), opt)\n",
    "            # delta = delta + alpha * jnp.sign(g)\n",
    "            # delta = l_inf_proj(delta, self.epsilon, self.cat_idx)\n",
    "        return params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfnet = CounterNetTrainingModule(\n",
    "    m_configs={\n",
    "        \"enc_sizes\": [50,10],\n",
    "        \"dec_sizes\": [10],\n",
    "        \"exp_sizes\": [50, 50]\n",
    "})\n",
    "dm = load_data('adult')\n",
    "params, _ = cfnet.init_net_opt(dm, random.PRNGKey(0))\n",
    "attack = RandomAttacker(\n",
    "# attack = BilevelAttacker(\n",
    "    keys=hk.PRNGSequence(0), \n",
    "    pred_loss_fn=cfnet.pred_loss_fn,\n",
    "    adv_loss_fn=cfnet.pred_loss_fn,\n",
    "    n_steps=10,\n",
    "    k=2,\n",
    "    epsilon=0.1,\n",
    "    adv_lr=0.01,\n",
    "    apply_fn=dm.apply_constraints,\n",
    "    cat_idx=dm.cat_idx, \n",
    "    check_assertions=True\n",
    ")\n",
    "_params = attack.step(params, random.PRNGKey(0), *next(iter(dm.train_dataloader(128))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RoCourseNetTrainingConfigs(CounterNetTrainingModuleConfigs):\n",
    "    epsilon: float = 0.1\n",
    "    n_steps: int = 7\n",
    "    k: int = 2\n",
    "    adv_lr: float\n",
    "    random_perturbation: bool = False\n",
    "    seed: int = 42\n",
    "\n",
    "    @property\n",
    "    def keys(self):\n",
    "        return hk.PRNGSequence(self.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RoCourseNetTrainingModule(CounterNetTrainingModule):\n",
    "    name = \"RoCourseNet\"\n",
    "\n",
    "    def __init__(self, m_configs: Dict[str, Any]):\n",
    "        super().__init__(m_configs)\n",
    "        self.configs = RoCourseNetTrainingConfigs(**m_configs)\n",
    "\n",
    "    def init_net_opt(self, data_module, key):\n",
    "        res = super().init_net_opt(data_module, key)\n",
    "        if self.configs.random_perturbation:\n",
    "            AdvCls = RandomAttacker\n",
    "        else:\n",
    "            AdvCls = BilevelAttacker\n",
    "        \n",
    "        self.attacker = AdvCls(\n",
    "            keys=hk.PRNGSequence(self.configs.seed), \n",
    "            pred_loss_fn=self.pred_loss_fn,\n",
    "            adv_loss_fn=self.adv_loss_fn,\n",
    "            n_steps=self.configs.n_steps,\n",
    "            k=self.configs.k,\n",
    "            epsilon=self.configs.epsilon,\n",
    "            adv_lr=self.configs.adv_lr,\n",
    "            apply_fn=self._data_module.apply_constraints,\n",
    "            cat_idx=self._data_module.cat_idx, \n",
    "        )\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def adv_loss_fn(\n",
    "        self,\n",
    "        params: hk.Params, \n",
    "        rng_key: random.PRNGKey, \n",
    "        x: jnp.DeviceArray, \n",
    "        is_training: bool = True\n",
    "    ):\n",
    "        y_pred, cf, cf_y = self.forward(params, rng_key, x, is_training)\n",
    "        y_prime = 1 - jnp.round(y_pred)\n",
    "        return self.loss_fn_2(cf_y, y_prime)\n",
    "\n",
    "    def bilevel_adv_step(\n",
    "        self,\n",
    "        params: hk.Params,\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.ndarray, jnp.ndarray]\n",
    "    ) -> hk.Params:\n",
    "        # if self.configs.random_perturbation:\n",
    "        #     AdvCls = RandomAttacker\n",
    "        # else:\n",
    "        #     AdvCls = BilevelAttacker\n",
    "\n",
    "        return self.attacker.step(params, rng_key, *batch)\n",
    "\n",
    "    def exp_loss_fn(\n",
    "        self,\n",
    "        trainable_params: hk.Params,\n",
    "        non_trainable_params: hk.Params,\n",
    "        aux_params: hk.Params,\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.DeviceArray, jnp.DeviceArray],\n",
    "        is_training: bool = True\n",
    "    ):\n",
    "        # merge trainable and non_trainable params\n",
    "        params = hk.data_structures.merge(trainable_params, non_trainable_params)\n",
    "        x, y = batch\n",
    "        y_pred, cf = self.net.apply(params, rng_key, x, is_training=is_training)\n",
    "        cf = self._data_module.apply_constraints(x, cf, hard=not is_training)\n",
    "        # compute cf_y on shifted model\n",
    "        cf_y, _ = self.net.apply(aux_params, rng_key, cf, is_training=is_training)\n",
    "        y_prime = 1 - jnp.round(y_pred)\n",
    "        loss_2, loss_3 = self.loss_fn_2(cf_y, y_prime), self.loss_fn_3(x, cf)\n",
    "        return self.configs.lambda_2 * loss_2 + self.configs.lambda_3 * loss_3\n",
    "\n",
    "    def explainer_step(self, params, aux_params, opt_state, rng_key, batch):\n",
    "        trainable_params, non_trainable_params = partition_trainable_params(\n",
    "            params, trainable_name='counter_net_model/Explainer'\n",
    "        )\n",
    "        grads = jax.grad(self.exp_loss_fn)(\n",
    "            trainable_params, non_trainable_params, aux_params, rng_key, batch)\n",
    "        upt_trainable_params, opt_state = grad_update(\n",
    "            grads, trainable_params, opt_state, self.opt_2)\n",
    "        upt_params = hk.data_structures.merge(upt_trainable_params, non_trainable_params)\n",
    "        return upt_params, opt_state\n",
    "\n",
    "    @partial(jax.jit, static_argnames=['self'])\n",
    "    def _training_step(self,\n",
    "        params: hk.Params,\n",
    "        opts_state: Tuple[optax.GradientTransformation, optax.GradientTransformation],\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array]\n",
    "    ):\n",
    "        opt_1_state, opt_2_state = opts_state\n",
    "        params, opt_1_state = self._predictor_step(params, opt_1_state, rng_key, batch)\n",
    "        aux_params = self.bilevel_adv_step(params, rng_key, batch)\n",
    "        upt_params, opt_2_state = self.explainer_step(params, aux_params, opt_2_state, rng_key, batch)\n",
    "        return upt_params, (opt_1_state, opt_2_state)\n",
    "\n",
    "    def _training_step_logs(self, params, rng_key, batch):\n",
    "        x, y = batch\n",
    "        logs = super()._training_step_logs(params, rng_key, batch)\n",
    "        adv_loss = self.adv_loss_fn(params, rng_key, x, is_training=False)\n",
    "        logs.update({\n",
    "            'train/adv_loss': adv_loss\n",
    "        })\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_configs = {\n",
    "    \"enc_sizes\": [50,10],\n",
    "    \"dec_sizes\": [10],\n",
    "    \"exp_sizes\": [50, 50],\n",
    "    \"dropout_rate\": 0.3,\n",
    "    'lr': 0.003,\n",
    "    \"lambda_1\": 1.0,\n",
    "    \"lambda_3\": 0.1,\n",
    "    \"lambda_2\": 0.2,\n",
    "    'adv_lr': 0.03\n",
    "}\n",
    "t_configs = {\n",
    "    'n_epochs': 2,\n",
    "    'monitor_metrics': 'val/val_loss',\n",
    "    'batch_size': 128,\n",
    "}\n",
    "data_configs = {\n",
    "    \"data_dir\": \"../assets/data/loan/year=2008.csv\",\n",
    "    \"data_name\": \"loan\",\n",
    "    'sample_frac': 0.1,\n",
    "    'batch_size': 128,\n",
    "    \"continous_cols\": [\n",
    "        \"NoEmp\", \"NewExist\", \"CreateJob\", \"RetainedJob\", \"DisbursementGross\", \"GrAppv\", \"SBA_Appv\"\n",
    "    ],\n",
    "    \"discret_cols\": [\n",
    "        \"State\", \"Term\", \"UrbanRural\", \"LowDoc\", \"Sector_Points\"\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.data import TabularDataModule, DataLoader\n",
    "from relax.trainer import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataModuleFast(TabularDataModule):\n",
    "    def train_dataloader(self, batch_size):\n",
    "        return DataLoader(self.train_dataset, self._configs.backend, \n",
    "            batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self, batch_size):\n",
    "        return DataLoader(self.val_dataset, self._configs.backend,\n",
    "            batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self, batch_size):\n",
    "        return DataLoader(self.val_dataset, self._configs.backend,\n",
    "            batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 16/16 [00:00<00:00, 50.15batch/s, train/adv_loss=0.10632468, train/train_loss_1=0.109, train/train_loss_2=0.106, train/train_loss_3=0.0489]  \n"
     ]
    }
   ],
   "source": [
    "dm = TabularDataModuleFast(data_configs)\n",
    "cfnet = RoCourseNetTrainingModule(m_configs)\n",
    "\n",
    "params, opt_state = train_model(\n",
    "    cfnet, dm, \n",
    "    t_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RoCourseNetConfigs(CounterNetConfigs):\n",
    "    epsilon: float = 0.1\n",
    "    n_steps: int = 7\n",
    "    k: int = 2\n",
    "    adv_lr: float = 0.03\n",
    "    random_perturbation: bool = False\n",
    "    seed: int = 42\n",
    "\n",
    "    @property\n",
    "    def keys(self):\n",
    "        return hk.PRNGSequence(self.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RoCourseNet(CounterNet):\n",
    "    name: str = \"RoCourseNet\"\n",
    "    \n",
    "    def __init__(self, m_configs: dict | RoCourseNetConfigs = None):\n",
    "        super().__init__(m_configs)\n",
    "        if m_configs is None:\n",
    "            m_configs = RoCourseNetConfigs()\n",
    "        self.module = RoCourseNetTrainingModule(m_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.evaluate import generate_cf_explanations\n",
    "from relax.trainer import train_model_with_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 16/16 [00:00<00:00, 49.53batch/s, train/adv_loss=0.13187183, train/train_loss_1=0.118, train/train_loss_2=0.132, train/train_loss_3=0.0441]\n"
     ]
    }
   ],
   "source": [
    "model = RoCourseNet(m_configs).module\n",
    "params, opt_state = model.init_net_opt(\n",
    "    dm, random.PRNGKey(42),\n",
    ")\n",
    "params, opt_state = train_model_with_states(\n",
    "    model, params, opt_state, dm, t_configs\n",
    ")\n",
    "\n",
    "# params, opt_state = train_model(\n",
    "#     RoCourseNet(m_configs).module, dm, \n",
    "#     t_configs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoCourseNet contains parametric models. Starts training before generating explanations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 7/7 [00:00<00:00, 44.45batch/s, train/adv_loss=0.15793148, train/train_loss_1=0.115, train/train_loss_2=0.158, train/train_loss_3=0.0458] \n"
     ]
    }
   ],
   "source": [
    "cf_exp = generate_cf_explanations(\n",
    "    RoCourseNet(m_configs),\n",
    "    dm,\n",
    "    t_configs=t_configs,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev2",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
