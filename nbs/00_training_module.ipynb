{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.counternet import CounterNetTrainingModule, CounterNetTrainingModuleConfigs, partition_trainable_params\n",
    "from relax.utils import grad_update\n",
    "from copy import deepcopy\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def l_inf_proj(x: jnp.ndarray, eps: float, cat_idx: Optional[int] = None):\n",
    "    if cat_idx is None:\n",
    "        return x.clip(-eps, eps)\n",
    "    else:\n",
    "        return jnp.concatenate([\n",
    "            x[:, :cat_idx].clip(-eps, eps), # clip continuous features only\n",
    "            x[:, cat_idx:]\n",
    "        ], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def filter_params(params):\n",
    "    return hk.data_structures.filter(\n",
    "        lambda m, n, v: m == 'counter_net_model/Predictor/dense_block/linear' and n == 'w', params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attack(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: hk.PRNGSequence,\n",
    "        pred_loss_fn: Callable[[hk.Params, random.PRNGKey, Tuple[jnp.DeviceArray, jnp.DeviceArray], bool], jnp.DeviceArray],\n",
    "        adv_loss_fn, # ignored \n",
    "        n_steps: int, # attacker steps\n",
    "        k: int, # inner steps\n",
    "        epsilon: float,\n",
    "        adv_lr: float,\n",
    "        apply_fn: Callable, # apply_fn(x, cf, hard=False)\n",
    "        cat_idx # ignored\n",
    "    ):\n",
    "        self.keys = keys\n",
    "        self.pred_loss_fn = pred_loss_fn\n",
    "        self.adv_loss_fn = adv_loss_fn\n",
    "        self.n_steps = n_steps\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.adv_lr = adv_lr\n",
    "        self.apply_fn = apply_fn\n",
    "        self.cat_idx = cat_idx\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        params: hk.Params,\n",
    "        x: jnp.ndarray, \n",
    "        y: jnp.ndarray,\n",
    "    ) -> hk.Params:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    __call__ = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RandomAttack(Attack):\n",
    "    def step(self, params, x, y):\n",
    "        # init delta randomly\n",
    "        delta = random.uniform(\n",
    "            key=next(self.keys), shape=x.shape, \n",
    "            minval=-self.epsilon, maxval=self.epsilon)\n",
    "        # create optimizer\n",
    "        opt = optax.adam(learning_rate=self.adv_lr)\n",
    "        opt_state = opt.init(params)\n",
    "\n",
    "        for _ in range(self.n_steps):\n",
    "            x = self.apply_fn(x, x + delta, hard=False)\n",
    "            for _ in range(self.k):\n",
    "                loss, grads = jax.value_and_grad(self.pred_loss_fn)(\n",
    "                        params, next(self.keys), (x, y), False)\n",
    "                params, opt_state = grad_update(grads, params, opt_state, opt)\n",
    "\n",
    "            delta = random.uniform(\n",
    "                key=next(self.keys), shape=x.shape, \n",
    "                minval=-self.epsilon, maxval=self.epsilon)\n",
    "            \n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilevelAttack(Attack):\n",
    "    def step(self, params: hk.Params, x: jnp.ndarray, y: jnp.ndarray) -> hk.Params:\n",
    "        # alpha is the delta's learning rate\n",
    "        alpha = self.epsilon * 2.5 / self.n_steps\n",
    "        # init delta randomly\n",
    "        delta = random.uniform(\n",
    "            key=next(self.keys), shape=x.shape, \n",
    "            minval=-self.epsilon, maxval=self.epsilon)\n",
    "        # create optimizer\n",
    "        opt = optax.adam(learning_rate=self.adv_lr)\n",
    "        opt_state = opt.init(params)\n",
    "\n",
    "        def attacker_fn(\n",
    "            delta: jnp.ndarray,\n",
    "            params: hk.Params,\n",
    "            opt_state: optax.OptState,\n",
    "            rng_key: random.PRNGKey,\n",
    "            batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
    "            opt: optax.GradientTransformation\n",
    "        ):\n",
    "            x, y = batch\n",
    "            for _ in range(self.k):\n",
    "                # inner unrolling steps\n",
    "                _x = self.apply_fn(x, x + delta, hard=False)\n",
    "                grads = jax.grad(self.pred_loss_fn)(params, rng_key, (_x, y), is_training=True)\n",
    "                params, opt_state = grad_update(grads, params, opt_state, opt)\n",
    "\n",
    "            # TODO: x or _x?\n",
    "            loss = self.adv_loss_fn(params, rng_key, x, is_training=False)\n",
    "            return loss, (params, opt_state)\n",
    "\n",
    "        for i in range(self.n_steps):\n",
    "            g, (params, opt_state) = jax.grad(attacker_fn, has_aux=True)(\n",
    "                delta, params, opt_state, next(self.keys), (x, y), opt)\n",
    "            delta = delta + alpha * jnp.sign(g)\n",
    "            delta = l_inf_proj(delta, self.epsilon, self.cat_idx)\n",
    "        return params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CounterNetTrainingModule' object has no attribute 'x_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27117/874564041.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcfnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounterNetTrainingModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounterNetTrainingModuleConfigs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcfnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_net_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'CounterNetTrainingModule' object has no attribute 'x_train'"
     ]
    }
   ],
   "source": [
    "# TODO: unit tests for attacks \n",
    "cfnet = CounterNetTrainingModule(CounterNetTrainingModuleConfigs())\n",
    "cfnet.init_net_opt(random.PRNGKey(0), cfnet.x_train[:1], cfnet.y_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def bilevel_adv_step(\n",
    "    params: hk.Params,\n",
    "    keys: hk.PRNGSequence,\n",
    "    batch: Tuple[jnp.DeviceArray, jnp.DeviceArray], # (x, y)\n",
    "    pred_loss_fn: Callable, \n",
    "    # pred_loss_fn(params, rng_key, batch, is_training=True)\n",
    "    adv_loss_fn:  Callable, \n",
    "    # adv_loss_fn(params, rng_key, x, is_training=True)\n",
    "    n_steps: int, # attacker steps\n",
    "    k: int, # inner steps\n",
    "    epsilon: float,\n",
    "    adv_lr: float,\n",
    "    apply_fn: Callable, # apply_fn(x, cf, hard=False)\n",
    "    cat_idx: int\n",
    "):\n",
    "    \"\"\"virtual data shift algorithm\"\"\"\n",
    "    x, y = batch\n",
    "    # alpha\n",
    "    alpha = epsilon * 2.5 / n_steps\n",
    "    # init delta randomly\n",
    "    delta = random.uniform(key=next(keys), shape=x.shape, minval=-epsilon, maxval=epsilon)\n",
    "    # create optimizer\n",
    "    opt = optax.adam(learning_rate=adv_lr)\n",
    "    opt_state = opt.init(params)\n",
    "    # val_loss = self.adv_loss_fn(params, rng_key, x, delta=None, is_training=False)\n",
    "\n",
    "    def attacker_fn(\n",
    "        delta: jnp.ndarray,\n",
    "        params: hk.Params,\n",
    "        opt_state: optax.OptState,\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
    "        opt: optax.GradientTransformation\n",
    "    ):\n",
    "        x, y = batch\n",
    "        for _ in range(k):\n",
    "            # inner unrolling steps\n",
    "            x = apply_fn(x, x + delta, hard=False)\n",
    "            grads = jax.grad(pred_loss_fn)(params, rng_key, batch, is_training=True)\n",
    "            params, opt_state = grad_update(grads, params, opt_state, opt)\n",
    "\n",
    "        loss = adv_loss_fn(params, rng_key, x, is_training=False)\n",
    "        return loss, (params, opt_state)\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        g, (params, opt_state) = jax.grad(attacker_fn, has_aux=True)(\n",
    "            delta, params, opt_state, next(keys), batch, opt)\n",
    "        delta = delta + alpha * jnp.sign(g)\n",
    "        delta = l_inf_proj(delta, epsilon, cat_idx)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RoCourseNetTrainingConfigs(CounterNetTrainingModuleConfigs):\n",
    "    epsilon: float = 0.1\n",
    "    n_steps: int = 7\n",
    "    k: int = 2\n",
    "    adv_lr: float\n",
    "    random_perturbation: bool = False\n",
    "    seed: int = 42\n",
    "\n",
    "    @property\n",
    "    def keys(self):\n",
    "        return hk.PRNGSequence(self.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RoCourseNetTrainingModule(CounterNetTrainingModule):\n",
    "    name = \"RoCourseNet\"\n",
    "\n",
    "    def __init__(self, m_configs: Dict[str, Any]):\n",
    "        super().__init__(m_configs)\n",
    "        self.configs = RoCourseNetTrainingConfigs(**m_configs)\n",
    "    \n",
    "    def adv_loss_fn(\n",
    "        self,\n",
    "        params: hk.Params, \n",
    "        rng_key: random.PRNGKey, \n",
    "        x: jnp.DeviceArray, \n",
    "        is_training: bool = True\n",
    "    ):\n",
    "        y_pred, cf, cf_y = self.forward(params, rng_key, x, is_training)\n",
    "        y_prime = 1 - jnp.round(y_pred)\n",
    "        return self.loss_fn_2(cf_y, y_prime)\n",
    "\n",
    "    def bilevel_adv_step(\n",
    "        self,\n",
    "        params: hk.Params,\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.ndarray, jnp.ndarray]\n",
    "    ) -> hk.Params:\n",
    "        if self.configs.random_perturbation:\n",
    "            adv_step_fn = random_adv_step\n",
    "        else:\n",
    "            adv_step_fn = bilevel_adv_step\n",
    "\n",
    "        return adv_step_fn(\n",
    "            params=params,\n",
    "            keys=rng_key,\n",
    "            batch=batch,\n",
    "            pred_loss_fn=self.pred_loss_fn,\n",
    "            adv_loss_fn=self.adv_loss_fn,\n",
    "            n_steps=self.configs.n_steps, \n",
    "            k=self.configs.k,\n",
    "            epsilon=self.configs.epsilon,\n",
    "            adv_lr=self.configs.adv_lr,\n",
    "            apply_fn=self._data_module.apply_constraints,\n",
    "            cat_idx=self._data_module.cat_idx\n",
    "        )\n",
    "\n",
    "    def exp_loss_fn(\n",
    "        self,\n",
    "        trainable_params: hk.Params,\n",
    "        non_trainable_params: hk.Params,\n",
    "        aux_params: hk.Params,\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.DeviceArray, jnp.DeviceArray],\n",
    "        is_training: bool = True\n",
    "    ):\n",
    "        # merge trainable and non_trainable params\n",
    "        params = hk.data_structures.merge(trainable_params, non_trainable_params)\n",
    "        x, y = batch\n",
    "        y_pred, cf = self.net.apply(params, rng_key, x, is_training=is_training)\n",
    "        cf = self._data_module.apply_constraints(x, cf, hard=not is_training)\n",
    "        # compute cf_y on shifted model\n",
    "        cf_y, _ = self.net.apply(aux_params, rng_key, cf, is_training=is_training)\n",
    "        y_prime = 1 - jnp.round(y_pred)\n",
    "        loss_2, loss_3 = self.loss_fn_2(cf_y, y_prime), self.loss_fn_3(x, cf)\n",
    "        return self.configs.lambda_2 * loss_2 + self.configs.lambda_3 * loss_3\n",
    "\n",
    "    def explainer_step(self, params, aux_params, opt_state, rng_key, batch):\n",
    "        trainable_params, non_trainable_params = partition_trainable_params(\n",
    "            params, trainable_name='counter_net_model/Explainer'\n",
    "        )\n",
    "        grads = jax.grad(self.exp_loss_fn)(\n",
    "            trainable_params, non_trainable_params, aux_params, rng_key, batch)\n",
    "        upt_trainable_params, opt_state = grad_update(\n",
    "            grads, trainable_params, opt_state, self.opt_2)\n",
    "        upt_params = hk.data_structures.merge(upt_trainable_params, non_trainable_params)\n",
    "        return upt_params, opt_state\n",
    "\n",
    "    @partial(jax.jit, static_argnames=['self'])\n",
    "    def _training_step(self,\n",
    "        params: hk.Params,\n",
    "        opts_state: Tuple[optax.GradientTransformation, optax.GradientTransformation],\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array]\n",
    "    ):\n",
    "        opt_1_state, opt_2_state = opts_state\n",
    "        params, opt_1_state = self._predictor_step(params, opt_1_state, rng_key, batch)\n",
    "        aux_params = self.bilevel_adv_step(params, rng_key, batch)\n",
    "        upt_params, opt_2_state = self.explainer_step(params, aux_params, opt_2_state, rng_key, batch)\n",
    "        return upt_params, (opt_1_state, opt_2_state)\n",
    "\n",
    "    def _training_step_logs(self, params, rng_key, batch):\n",
    "        x, y = batch\n",
    "        logs = super()._training_step_logs(params, rng_key, batch)\n",
    "        adv_loss = self.adv_loss_fn(params, rng_key, x, is_training=False)\n",
    "        logs.update({\n",
    "            'train/adv_loss': adv_loss\n",
    "        })\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_configs = {\n",
    "    \"enc_sizes\": [50,10],\n",
    "    \"dec_sizes\": [10],\n",
    "    \"exp_sizes\": [50, 50],\n",
    "    \"dropout_rate\": 0.3,\n",
    "    'lr': 0.003,\n",
    "    \"lambda_1\": 1.0,\n",
    "    \"lambda_3\": 0.1,\n",
    "    \"lambda_2\": 0.2,\n",
    "}\n",
    "t_configs = {\n",
    "    'n_epochs': 100,\n",
    "    'monitor_metrics': 'val/val_loss'\n",
    "}\n",
    "data_configs = {\n",
    "    \"data_dir\": \"../assets/data/loan/year=2009.csv\",\n",
    "    \"data_name\": \"adult\",\n",
    "    \"batch_size\": 128,\n",
    "    'sample_frac': 0.1,\n",
    "    \"continous_cols\": [\n",
    "        \"NoEmp\", \"NewExist\", \"CreateJob\", \"RetainedJob\", \"DisbursementGross\", \"GrAppv\", \"SBA_Appv\"\n",
    "    ],\n",
    "    \"discret_cols\": [\n",
    "        \"State\", \"Term\", \"UrbanRural\", \"LowDoc\", \"Sector_Points\"\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 7/7 [00:00<00:00, 92.79batch/s, train/train_loss_1=0.030241895, train/train_loss_2=1.4216381e-07, train/train_loss_3=0.030580275]\n"
     ]
    }
   ],
   "source": [
    "from cfnet.datasets import TabularDataModule\n",
    "from cfnet.train import train_model\n",
    "\n",
    "\n",
    "dm = TabularDataModule(data_configs)\n",
    "cfnet = RoCourseNetTrainingModule(m_configs)\n",
    "\n",
    "params, opt_state = train_model(\n",
    "    cfnet, dm, \n",
    "    t_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>validity</th>\n",
       "      <th>proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RoCourseNet</th>\n",
       "      <td>0.7820559</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.7352805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   acc validity  proximity\n",
       "RoCourseNet  0.7820559      1.0  6.7352805"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cfnet.evaluate import evaluate_cfs\n",
    "\n",
    "cf_res = cfnet.generate_cf_results(dm, params, random.PRNGKey(0))\n",
    "evaluate_cfs(\n",
    "    cf_res, return_df=True\n",
    ")[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev2",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
