{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "{}\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from cfnet.import_essentials import *\n",
    "from cfnet.datasets import TabularDataModule\n",
    "from cfnet.train import train_model, train_model_with_states\n",
    "from cfnet.evaluate import evaluate_cfs, benchmark_cfs, generate_cf_results, generate_cf_results_local_exp, generate_cf_results_cfnet, CFExplanationResults\n",
    "from cfnet.training_module import BaseTrainingModule, CounterNetTrainingModule\n",
    "from cfnet.interfaces import BaseCFExplanationModule, LocalCFExplanationModule\n",
    "from cfnet.utils import accuracy\n",
    "from rocourset_net.training_module import RoCourseNetTrainingModule\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,OneHotEncoder\n",
    "import wandb\n",
    "from pydantic import validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_rob_validity(cf_results: CFExplanationResults, shifted_pred_fn: Callable):\n",
    "    pred_fn = cf_results.pred_fn\n",
    "    y_pred = pred_fn(cf_results.X).reshape(-1, 1).round()\n",
    "    y_prime = 1. - y_pred\n",
    "    cf_y = shifted_pred_fn(cf_results.cfs).reshape(-1, 1).round()\n",
    "\n",
    "    rob_validity = accuracy(y_prime, cf_y)\n",
    "    return rob_validity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _aggregate_default_data_encoders(default_data_config: Dict[str, Any], data_dir_list: List[str]):\n",
    "    # data encoding\n",
    "    data = pd.concat(\n",
    "        [pd.read_csv(data_dir) for data_dir in data_dir_list]\n",
    "    )\n",
    "    print(f\"total data length: {len(data)}\")\n",
    "    if len(default_data_config['continous_cols']) != 0:\n",
    "        print(\"preprocessing continuous features...\")\n",
    "        normalizer = MinMaxScaler().fit(\n",
    "            data[default_data_config['continous_cols']]\n",
    "        )\n",
    "        default_data_config.update({\"normalizer\": normalizer})\n",
    "\n",
    "    if len(default_data_config['discret_cols']) != 0:\n",
    "        print(\"preprocessing discret features...\")\n",
    "        encoder = OneHotEncoder(sparse=False).fit(\n",
    "            data[default_data_config['discret_cols']]\n",
    "        )\n",
    "        default_data_config.update({\"encoder\": encoder})\n",
    "    return default_data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_validity_matrix(\n",
    "    cf_results_list: Iterable[CFExplanationResults]\n",
    ") -> pd.DataFrame:\n",
    "    validity_matrix_dict = {}\n",
    "    for i, cf_results_i in enumerate(cf_results_list):\n",
    "        data_name = cf_results_i.dataset_name\n",
    "        rob_validity = {}\n",
    "        for j, cf_results_j in enumerate(cf_results_list):\n",
    "            cf_name_j = cf_results_j.cf_name\n",
    "            shifted_pred_fn = cf_results_j.pred_fn\n",
    "            val = compute_rob_validity(\n",
    "                cf_results_i, shifted_pred_fn\n",
    "            )\n",
    "            rob_validity[cf_name_j] = val\n",
    "            # print(f'data_name: {data_name}; cf_name: {cf_name_j}; shifted_pred_fn: {shifted_pred_fn}; val: {val} ')\n",
    "        validity_matrix_dict[data_name] = rob_validity\n",
    "    return pd.DataFrame.from_dict(validity_matrix_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_models(\n",
    "    training_module: BaseTrainingModule,\n",
    "    default_data_configs: Dict[str, Any],\n",
    "    data_dir_list: List[str],\n",
    "    t_configs: Dict[str, Any],\n",
    "    return_data_module_list: bool = False\n",
    "):\n",
    "    model_params_opt_list = []\n",
    "    data_module_list = []\n",
    "    for i, data_dir in enumerate(data_dir_list):\n",
    "        data_config = deepcopy(default_data_configs)\n",
    "        data_config['data_dir'] = data_dir\n",
    "        dm = TabularDataModule(data_config)\n",
    "        params, opt_state = training_module.init_net_opt(\n",
    "            dm, random.PRNGKey(0)\n",
    "        )\n",
    "\n",
    "        params, opt_state = train_model_with_states(\n",
    "            training_module, params, opt_state, dm, t_configs\n",
    "        )\n",
    "        model_params_opt_list.append((params, opt_state))\n",
    "        data_module_list.append(dm)\n",
    "\n",
    "    if return_data_module_list:\n",
    "        return model_params_opt_list, data_module_list\n",
    "    else:\n",
    "        return model_params_opt_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log to Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def wandb_vis_table(table: pd.DataFrame):\n",
    "    return wandb.Table(dataframe=table)\n",
    "\n",
    "def wandb_vis_heatmap(heatmap: pd.DataFrame):\n",
    "    return wandb.plots.HeatMap(x_labels=heatmap.columns, y_labels=heatmap.index, matrix_values=heatmap.values, show_text=False)\n",
    "\n",
    "DATA_TYPE_TO_VIS_FN = {\n",
    "    'table': wandb_vis_table,\n",
    "    'heatmap': wandb_vis_heatmap\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class ExperimentResult(BaseParser):\n",
    "    name: str\n",
    "    data_type: str\n",
    "    data: Any\n",
    "\n",
    "    @validator('data_type')\n",
    "    def validate_data_type(cls, v):\n",
    "        if v not in DATA_TYPE_TO_VIS_FN.keys():\n",
    "            raise ValueError(f\"`data_type` should be one of {DATA_TYPE_TO_VIS_FN.keys()}, but got {v}\")\n",
    "        return v\n",
    "\n",
    "    def wandb_vis(self):\n",
    "        return DATA_TYPE_TO_VIS_FN[self.data_type](self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ExperimentLogger(ABC):\n",
    "    @abstractmethod\n",
    "    def store_results(self, results: List[ExperimentResult]):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ExperimentLoggerWanbConfigs(BaseParser):\n",
    "    project_name: str                           # `project`\n",
    "    user_name: str                              # `entity`\n",
    "    experiment_name: str                        # `name`\n",
    "    hparams: Optional[Dict[str, Any]] = None    # hypterparamters\n",
    "\n",
    "\n",
    "class ExperimentLoggerWanb(ExperimentLogger):\n",
    "    def __init__(self, configs: ExperimentLoggerWanbConfigs):\n",
    "        super().__init__()\n",
    "        self.run = wandb.init(\n",
    "            project=configs.project_name, entity=configs.user_name, name=configs.experiment_name, config=configs.hparams,\n",
    "            settings=wandb.Settings(start_method=\"fork\"))\n",
    "\n",
    "    def store_results(self, results: List[ExperimentResult]):\n",
    "        with self.run as run:\n",
    "            run.log({\n",
    "                r.name: r.wandb_vis() for r in results\n",
    "            })\n",
    "        return self.run.dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def calculate_validity_changes(val_matrix_df: pd.DataFrame):\n",
    "    assert len(val_matrix_df.columns) == len(val_matrix_df.index), \\\n",
    "        f\"val_matrix_df.columns={val_matrix_df.columns}, but val_matrix_df.index={val_matrix_df.index}\"\n",
    "    matrix = val_matrix_df.values\n",
    "    n_datasets = len(matrix[0])\n",
    "\n",
    "    validity = [matrix[i][i] for i in range(n_datasets)]\n",
    "    w1_val_result, w1_dec_result = [], []\n",
    "    for i in range(n_datasets - 1):\n",
    "        w1_val_result.append(matrix[i][i+1])\n",
    "        w1_dec_result.append(matrix[i][i] - matrix[i][i+1])\n",
    "\n",
    "    wall_val_result, wall_dec_result =  [], []\n",
    "    for i in range(n_datasets):\n",
    "        for j in range(n_datasets):\n",
    "            if j == i: continue\n",
    "            wall_val_result.append(matrix[i][j])\n",
    "            wall_dec_result.append(matrix[i][i] - matrix[i][j])\n",
    "    result = {\n",
    "        'cf_validity': { 'mean': np.average(validity), 'std': np.std(validity) },\n",
    "        'cf_validity (w=1)': { 'mean': np.average(w1_val_result), 'std': np.std(w1_val_result) },\n",
    "        'cf_validity (all)': { 'mean': np.average(wall_val_result), 'std': np.std(wall_val_result) },\n",
    "        'validity_decrease (w=1)': { 'mean': np.average(w1_dec_result), 'std': np.std(w1_dec_result) },\n",
    "        'validity_decrease (all)': { 'mean': np.average(wall_dec_result), 'std': np.std(wall_dec_result) }\n",
    "    }\n",
    "    return pd.DataFrame.from_dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _evaluate_adversarial_model(\n",
    "    cf_results_list: Iterable[CFExplanationResults],\n",
    "    experiment_logger_configs: Optional[ExperimentLoggerWanbConfigs] = None\n",
    "):\n",
    "    cf_results_df = benchmark_cfs(cf_results_list)\n",
    "    cf_aggre_df = cf_results_df.describe().loc[['mean', 'std']].reset_index().rename(columns={'index': 'stat'})\n",
    "\n",
    "    print(\"calculating the validity matrix...\")\n",
    "    validity_matrix_df = calculate_validity_matrix(cf_results_list=cf_results_list)\n",
    "    valditity_changes = calculate_validity_changes(validity_matrix_df)\n",
    "\n",
    "    experiment_results = [\n",
    "        ExperimentResult(name='CF Results', data_type='table', data=cf_results_df),\n",
    "        ExperimentResult(name='CF Metrics', data_type='table', data=cf_aggre_df),\n",
    "        ExperimentResult(name='Heatmap', data_type='heatmap', data=validity_matrix_df),\n",
    "        ExperimentResult(name='Validity Matrix', data_type='table', data=validity_matrix_df),\n",
    "        ExperimentResult(name='Validity Changes', data_type='table', data=valditity_changes),\n",
    "    ]\n",
    "\n",
    "    if experiment_logger_configs:\n",
    "        logger = ExperimentLoggerWanb(experiment_logger_configs)\n",
    "        dir_path = logger.store_results(experiment_results)\n",
    "        print(f\"Results stored at {dir_path}\")\n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def adversarial_experiment(\n",
    "    pred_training_module: BaseTrainingModule,\n",
    "    cf_module: BaseCFExplanationModule,\n",
    "    default_data_config: Dict[str, Any],\n",
    "    data_dir_list: List[str],\n",
    "    t_config: Dict[str, Any],\n",
    "    is_local_cf_module: bool,\n",
    "    use_prev_model_params: bool = False,\n",
    "    return_best_model: bool = False, # return last model by default\n",
    "    experiment_logger_configs: Optional[ExperimentLoggerWanbConfigs] = None\n",
    "):\n",
    "    hparams = deepcopy(default_data_config)\n",
    "    hparams.update(pred_training_module.hparams)\n",
    "    hparams.update(t_config)\n",
    "    if experiment_logger_configs:\n",
    "        experiment_logger_configs.hparams = hparams\n",
    "\n",
    "    # data encoding\n",
    "    print(\"aggregating data...\")\n",
    "    default_data_config = _aggregate_default_data_encoders(default_data_config, data_dir_list)\n",
    "\n",
    "    # training models\n",
    "    print(\"start training...\")\n",
    "    model_params_opt_list, data_module_list = train_models(\n",
    "        pred_training_module, default_data_config,\n",
    "        data_dir_list, t_config, return_data_module_list=True\n",
    "    )\n",
    "\n",
    "    # evaluate cfs\n",
    "    print(\"generating cfs...\")\n",
    "    experiment_results: List[ExperimentResult] = []\n",
    "    cf_results_list = []\n",
    "\n",
    "    # generate_cf_results_fn\n",
    "    def generate_cf_results_fn(cf_module, dm, params, rng_key):\n",
    "        _params = deepcopy(params)\n",
    "        pred_fn = lambda x: pred_training_module.forward(_params, rng_key, x)\n",
    "        return generate_cf_results_local_exp(cf_module, dm, pred_fn=pred_fn) if is_local_cf_module \\\n",
    "            else generate_cf_results_cfnet(cf_module, dm, params=_params, rng_key=rng_key)\n",
    "\n",
    "    for i, ((params, _), dm) in enumerate(zip(model_params_opt_list, data_module_list)):\n",
    "        cf_results = generate_cf_results_fn(\n",
    "            cf_module, dm,\n",
    "            params=params, rng_key=random.PRNGKey(0)\n",
    "        )\n",
    "        cf_results.cf_name = f\"model_{i}\"\n",
    "        cf_results.dataset_name = f\"data_{i}\"\n",
    "        cf_results_list.append(cf_results)\n",
    "\n",
    "    experiment_results = _evaluate_adversarial_model(\n",
    "        cf_results_list, experiment_logger_configs=experiment_logger_configs)\n",
    "    return cf_results_list, experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def adversarial_experiment_cfnet(\n",
    "    training_module: CounterNetTrainingModule,\n",
    "    default_data_config: Dict[str, Any],\n",
    "    data_dir_list: List[str],\n",
    "    t_config: Dict[str, Any],\n",
    "    use_prev_model_params: bool = False,\n",
    "    return_best_model: bool = False, # return last model by default\n",
    "    experiment_logger_configs: Optional[ExperimentLoggerWanbConfigs] = None\n",
    "):\n",
    "    return adversarial_experiment(\n",
    "        pred_training_module=training_module,\n",
    "        cf_module=training_module,\n",
    "        default_data_config=default_data_config,\n",
    "        data_dir_list=data_dir_list,\n",
    "        t_config=t_config,\n",
    "        is_local_cf_module=False,\n",
    "        use_prev_model_params=use_prev_model_params,\n",
    "        return_best_model=return_best_model,\n",
    "        experiment_logger_configs=experiment_logger_configs\n",
    "    )\n",
    "\n",
    "def adversarial_experiment_local_exp(\n",
    "    pred_training_module: CounterNetTrainingModule,\n",
    "    cf_moudle: LocalCFExplanationModule,\n",
    "    default_data_config: Dict[str, Any],\n",
    "    data_dir_list: List[str],\n",
    "    t_config: Dict[str, Any],\n",
    "    use_prev_model_params: bool = False,\n",
    "    return_best_model: bool = False, # return last model by default\n",
    "    experiment_logger_configs: Optional[ExperimentLoggerWanbConfigs] = None\n",
    "):\n",
    "    return adversarial_experiment(\n",
    "        pred_training_module=pred_training_module,\n",
    "        cf_module=cf_moudle,\n",
    "        default_data_config=default_data_config,\n",
    "        data_dir_list=data_dir_list,\n",
    "        t_config=t_config,\n",
    "        is_local_cf_module=True,\n",
    "        use_prev_model_params=use_prev_model_params,\n",
    "        return_best_model=return_best_model,\n",
    "        experiment_logger_configs=experiment_logger_configs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "cf_results_list = [\n",
    "    CFExplanationResults(\n",
    "        cf_name='m1',\n",
    "        dataset_name='d1',\n",
    "        X=jnp.array([\n",
    "            [1, 0, 1],\n",
    "            [1, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 1],\n",
    "        ]), # y_pred = [1, 1, 0, 0]\n",
    "        y=jnp.ones((4,1)),\n",
    "        cfs=jnp.array([\n",
    "            [0, 0, 1],\n",
    "            [0, 1, 0],\n",
    "            [1, 1, 0],\n",
    "            [1, 1, 1]\n",
    "        ]),\n",
    "        pred_fn=lambda x: x[:, 0], \n",
    "        total_time=0.1\n",
    "    ),\n",
    "    CFExplanationResults(\n",
    "        cf_name='m2',\n",
    "        dataset_name='d2',\n",
    "        X=jnp.array([\n",
    "            [1, 0, 1],\n",
    "            [1, 1, 0],\n",
    "            [0, 1, 1],\n",
    "            [1, 0, 0]\n",
    "        ]), # y_pred = [1, 0, 1, 0]\n",
    "        y=jnp.ones((4,1)),\n",
    "        cfs=jnp.array([\n",
    "            [1, 0, 0],\n",
    "            [1, 1, 1],\n",
    "            [0, 1, 0],\n",
    "            [1, 0, 1]\n",
    "        ]),\n",
    "        pred_fn=lambda x: x[:, -1],\n",
    "        total_time=0.1\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert compute_rob_validity(cf_results_list[0], cf_results_list[0].pred_fn) == 1.0\n",
    "assert compute_rob_validity(cf_results_list[0], cf_results_list[1].pred_fn) == 0.5\n",
    "assert compute_rob_validity(cf_results_list[1], cf_results_list[0].pred_fn) == 0.75\n",
    "assert compute_rob_validity(cf_results_list[1], cf_results_list[1].pred_fn) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>m1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     d1    d2\n",
       "m1  1.0  0.75\n",
       "m2  0.5  1.00"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_validity_matrix(cf_results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    \"data_name\": \"loan\",\n",
    "    \"continous_cols\": [\n",
    "        \"NoEmp\", \"NewExist\", \"CreateJob\", \"RetainedJob\", \"DisbursementGross\", \"GrAppv\", \"SBA_Appv\"\n",
    "    ],\n",
    "    \"discret_cols\": [\n",
    "        \"State\", \"Term\", \"UrbanRural\", \"LowDoc\", \"Sector_Points\"\n",
    "    ],\n",
    "    \"batch_size\": 128,\n",
    "    'sample_frac': 0.1,\n",
    "}\n",
    "m_config = {\n",
    "    \"enc_sizes\": [200,10],\n",
    "    \"dec_sizes\": [10],\n",
    "    \"exp_sizes\": [50],\n",
    "    \"dropout_rate\": 0.3,    \n",
    "    'lr': 0.003,\n",
    "    \"lambda_1\": 1.0,\n",
    "    \"lambda_3\": 0.1,\n",
    "    \"lambda_2\": 0.2,\n",
    "}\n",
    "t_configs = {\n",
    "    # 'n_epochs': 100,\n",
    "    'n_epochs': 10,\n",
    "    'monitor_metrics': 'val/val_loss'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregating data...\n",
      "total data length: 38341\n",
      "preprocessing continuous features...\n",
      "preprocessing discret features...\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 17/17 [00:00<00:00, 52.24batch/s, train/train_loss_1=0.38231245, train/train_loss_2=4.258957e-09, train/train_loss_3=0.051329758]   \n",
      "Epoch 9: 100%|██████████| 7/7 [00:00<00:00, 61.73batch/s, train/train_loss_1=0.068208955, train/train_loss_2=0.120453365, train/train_loss_3=0.04799317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating cfs...\n",
      "calculating the validity matrix...\n"
     ]
    }
   ],
   "source": [
    "experiment_logger_configs = ExperimentLoggerWanbConfigs(\n",
    "    project_name='debug',\n",
    "    user_name='birkhoffg',\n",
    "    experiment_name='rocoursenet',\n",
    "    \n",
    ")\n",
    "cf_results_list, experiment_results = adversarial_experiment(\n",
    "    training_module=RoCourseNetTrainingModule(m_config),\n",
    "    default_data_config=data_config,\n",
    "    data_dir_list=[ \n",
    "        f\"assets/data/loan/year={year}.csv\" for year in range(2008, 2010) \n",
    "    ],\n",
    "    # data_dir_list=[ \n",
    "    #     f\"assets/data/loan/year={year}.csv\" for year in range(1994, 2010) \n",
    "    # ],\n",
    "    t_config=t_configs,\n",
    "    # experiment_logger_configs=experiment_logger_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_0</th>\n",
       "      <th>data_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model_0</th>\n",
       "      <td>0.994878</td>\n",
       "      <td>0.995641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_1</th>\n",
       "      <td>0.695449</td>\n",
       "      <td>0.994915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           data_0    data_1\n",
       "model_0  0.994878  0.995641\n",
       "model_1  0.695449  0.994915"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_validity_matrix(cf_results_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
