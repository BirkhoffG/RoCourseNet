{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from relax.import_essentials import *\n",
    "from relax.data import TabularDataModule, DataLoader\n",
    "from relax.trainer import train_model_with_states, TrainingConfigs\n",
    "from relax.evaluate import (\n",
    "    Explanation,\n",
    "    accuracy,\n",
    "    evaluate_cfs,\n",
    "    benchmark_cfs,\n",
    "    generate_cf_explanations,\n",
    "    _AuxPredFn,\n",
    "    BaseEvalMetrics,\n",
    "    Validity,\n",
    "    Proximity,\n",
    "    PredictiveAccuracy\n",
    ")\n",
    "from relax.module import BaseTrainingModule\n",
    "from relax.methods.base import BaseCFModule, BaseParametricCFModule, BasePredFnCFModule\n",
    "from relax.utils import validate_configs, proximity\n",
    "\n",
    "from rocourse_net.module import RoCourseNetTrainingModule\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "import wandb\n",
    "from pydantic import validator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_rob_validity(cf_results: Explanation, shifted_pred_fn: Callable):\n",
    "    pred_fn = cf_results.pred_fn\n",
    "    y_pred = pred_fn(cf_results.X).reshape(-1, 1).round()\n",
    "    y_prime = 1. - y_pred\n",
    "    cf_y = shifted_pred_fn(cf_results.cfs).reshape(-1, 1).round()\n",
    "\n",
    "    rob_validity = accuracy(y_prime, cf_y)\n",
    "    return rob_validity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _aggregate_default_data_encoders(default_data_config: Dict[str, Any], data_dir_list: List[str]):\n",
    "    # data encoding\n",
    "    data = pd.concat(\n",
    "        [pd.read_csv(data_dir) for data_dir in data_dir_list]\n",
    "    )\n",
    "    print(f\"total data length: {len(data)}\")\n",
    "    if len(default_data_config['continous_cols']) != 0:\n",
    "        print(\"preprocessing continuous features...\")\n",
    "        normalizer = MinMaxScaler().fit(\n",
    "            data[default_data_config['continous_cols']]\n",
    "        )\n",
    "        default_data_config.update({\"normalizer\": normalizer})\n",
    "\n",
    "    if len(default_data_config['discret_cols']) != 0:\n",
    "        print(\"preprocessing discret features...\")\n",
    "        encoder = OneHotEncoder(sparse=False).fit(\n",
    "            data[default_data_config['discret_cols']]\n",
    "        )\n",
    "        default_data_config.update({\"encoder\": encoder})\n",
    "    return default_data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_validity_matrix(\n",
    "    cf_results_list: Iterable[Explanation]\n",
    ") -> pd.DataFrame:\n",
    "    validity_matrix_dict = {}\n",
    "    for i, cf_results_i in enumerate(cf_results_list):\n",
    "        data_name = cf_results_i.dataset_name\n",
    "        rob_validity = {}\n",
    "        for j, cf_results_j in enumerate(cf_results_list):\n",
    "            cf_name_j = cf_results_j.cf_name\n",
    "            shifted_pred_fn = cf_results_j.pred_fn\n",
    "            val = compute_rob_validity(\n",
    "                cf_results_i, shifted_pred_fn\n",
    "            )\n",
    "            rob_validity[cf_name_j] = val\n",
    "            # print(f'data_name: {data_name}; cf_name: {cf_name_j}; shifted_pred_fn: {shifted_pred_fn}; val: {val} ')\n",
    "        validity_matrix_dict[data_name] = rob_validity\n",
    "    return pd.DataFrame.from_dict(validity_matrix_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_datamodules(\n",
    "    default_data_configs: Dict[str, Any],\n",
    "    data_dir_list: List[str]\n",
    "):\n",
    "    data_module_list = []\n",
    "    for data_dir in data_dir_list:\n",
    "        data_config = deepcopy(default_data_configs)\n",
    "        data_config['data_dir'] = data_dir\n",
    "        data_module = TabularDataModule(data_config)\n",
    "        data_module_list.append(data_module)\n",
    "    return data_module_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FasterTabularDataModule(TabularDataModule):\n",
    "    def train_dataloader(self, batch_size):\n",
    "        return DataLoader(self.train_dataset, self._configs.backend, \n",
    "            batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self, batch_size):\n",
    "        return DataLoader(self.val_dataset, self._configs.backend,\n",
    "            batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self, batch_size):\n",
    "        return DataLoader(self.val_dataset, self._configs.backend,\n",
    "            batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_models(\n",
    "    training_module: BaseTrainingModule,\n",
    "    default_data_configs: Dict[str, Any],\n",
    "    data_dir_list: List[str],\n",
    "    t_configs: Dict[str, Any],\n",
    "    return_data_module_list: bool = False,\n",
    "    use_fast: bool = True\n",
    "):\n",
    "    model_params_opt_list = []\n",
    "    data_module_list = []\n",
    "\n",
    "    for i, data_dir in enumerate(data_dir_list):\n",
    "        data_config = deepcopy(default_data_configs)\n",
    "        data_config['data_dir'] = data_dir\n",
    "        if use_fast:\n",
    "            dm = FasterTabularDataModule(data_config)\n",
    "        else:\n",
    "            dm = TabularDataModule(data_config)\n",
    "        params, opt_state = training_module.init_net_opt(\n",
    "            dm, random.PRNGKey(42)\n",
    "        )\n",
    "        \n",
    "        params, opt_state = train_model_with_states(\n",
    "            training_module, params, opt_state, dm, t_configs\n",
    "        )\n",
    "        \n",
    "        model_params_opt_list.append((params, opt_state))\n",
    "        data_module_list.append(dm)\n",
    "    if return_data_module_list:\n",
    "        return model_params_opt_list, data_module_list\n",
    "    else:\n",
    "        return model_params_opt_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log to Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def wandb_vis_table(table: pd.DataFrame):\n",
    "    return wandb.Table(dataframe=table)\n",
    "\n",
    "def wandb_vis_heatmap(heatmap: pd.DataFrame):\n",
    "    return wandb.plots.HeatMap(x_labels=heatmap.columns, y_labels=heatmap.index, matrix_values=heatmap.values, show_text=False)\n",
    "\n",
    "DATA_TYPE_TO_VIS_FN = {\n",
    "    'table': wandb_vis_table,\n",
    "    'heatmap': wandb_vis_heatmap\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class ExperimentResult(BaseParser):\n",
    "    name: str\n",
    "    data_type: str\n",
    "    data: Any\n",
    "\n",
    "    @validator('data_type')\n",
    "    def validate_data_type(cls, v):\n",
    "        if v not in DATA_TYPE_TO_VIS_FN.keys():\n",
    "            raise ValueError(f\"`data_type` should be one of {DATA_TYPE_TO_VIS_FN.keys()}, but got {v}\")\n",
    "        return v\n",
    "\n",
    "    def wandb_vis(self):\n",
    "        return DATA_TYPE_TO_VIS_FN[self.data_type](self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ExperimentLogger(ABC):\n",
    "    @abstractmethod\n",
    "    def store_results(self, results: List[ExperimentResult]):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ExperimentLoggerWanbConfigs(BaseParser):\n",
    "    project_name: str                           # `project`\n",
    "    user_name: str                              # `entity`\n",
    "    experiment_name: str                        # `name`\n",
    "    hparams: Optional[Dict[str, Any]] = None    # hypterparamters\n",
    "\n",
    "\n",
    "class ExperimentLoggerWanb(ExperimentLogger):\n",
    "    def __init__(self, configs: ExperimentLoggerWanbConfigs):\n",
    "        super().__init__()\n",
    "        self.run = wandb.init(\n",
    "            project=configs.project_name, entity=configs.user_name, name=configs.experiment_name, config=configs.hparams,\n",
    "            settings=wandb.Settings(start_method=\"fork\"))\n",
    "\n",
    "    def store_results(self, results: List[ExperimentResult]):\n",
    "        with self.run as run:\n",
    "            run.log({\n",
    "                r.name: r.wandb_vis() for r in results\n",
    "            })\n",
    "        return self.run.dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NormalizedProximity(BaseEvalMetrics):\n",
    "    def __str__(self):\n",
    "        return \"NormalizedProximity\"\n",
    "    \n",
    "    \"\"\"Normalized proximity of counterfactuals to the original instance.\"\"\"\n",
    "    def __call__(self, cf_explanations: Explanation) -> float:\n",
    "        X, _ = cf_explanations.data_module.test_dataset[:]\n",
    "        return (proximity(X, cf_explanations.cfs) / X.shape[1]).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def calculate_validity_changes(val_matrix_df: pd.DataFrame):\n",
    "    assert len(val_matrix_df.columns) == len(val_matrix_df.index), \\\n",
    "        f\"val_matrix_df.columns={val_matrix_df.columns}, but val_matrix_df.index={val_matrix_df.index}\"\n",
    "    matrix = val_matrix_df.values\n",
    "    n_datasets = len(matrix[0])\n",
    "\n",
    "    validity = [matrix[i][i] for i in range(n_datasets)]\n",
    "    w1_val_result, w1_dec_result = [], []\n",
    "    for i in range(n_datasets - 1):\n",
    "        w1_val_result.append(matrix[i][i+1])\n",
    "        w1_dec_result.append(matrix[i][i] - matrix[i][i+1])\n",
    "\n",
    "    wall_val_result, wall_dec_result =  [], []\n",
    "    for i in range(n_datasets):\n",
    "        for j in range(n_datasets):\n",
    "            if j == i: continue\n",
    "            wall_val_result.append(matrix[i][j])\n",
    "            wall_dec_result.append(matrix[i][i] - matrix[i][j])\n",
    "    result = {\n",
    "        'cf_validity': { 'mean': np.average(validity), 'std': np.std(validity) },\n",
    "        'cf_validity (w=1)': { 'mean': np.average(w1_val_result), 'std': np.std(w1_val_result) },\n",
    "        'cf_validity (all)': { 'mean': np.average(wall_val_result), 'std': np.std(wall_val_result) },\n",
    "        'validity_decrease (w=1)': { 'mean': np.average(w1_dec_result), 'std': np.std(w1_dec_result) },\n",
    "        'validity_decrease (all)': { 'mean': np.average(wall_dec_result), 'std': np.std(wall_dec_result) }\n",
    "    }\n",
    "    return pd.DataFrame.from_dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def _evaluate_adversarial_model(\n",
    "    cf_results_list: Iterable[Explanation],\n",
    "    experiment_logger_configs: Optional[ExperimentLoggerWanbConfigs] = None,\n",
    "):\n",
    "    cf_results_df = benchmark_cfs(\n",
    "        cf_results_list,\n",
    "        # metrics=[\n",
    "        #     PredictiveAccuracy(),\n",
    "        #     Validity(),\n",
    "        #     NormalizedProximity(),\n",
    "        # ],\n",
    "    )\n",
    "    cf_aggre_df = (\n",
    "        cf_results_df.describe()\n",
    "        .loc[[\"mean\", \"std\"]]\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"stat\"})\n",
    "    )\n",
    "\n",
    "    print(\"calculating the validity matrix...\")\n",
    "    validity_matrix_df = calculate_validity_matrix(cf_results_list=cf_results_list)\n",
    "    valditity_changes = calculate_validity_changes(validity_matrix_df)\n",
    "\n",
    "    experiment_results = [\n",
    "        ExperimentResult(name=\"CF Results\", data_type=\"table\", data=cf_results_df),\n",
    "        ExperimentResult(name=\"CF Metrics\", data_type=\"table\", data=cf_aggre_df),\n",
    "        ExperimentResult(name=\"Heatmap\", data_type=\"heatmap\", data=validity_matrix_df),\n",
    "        ExperimentResult(name=\"Validity Matrix\", data_type=\"table\", data=validity_matrix_df),\n",
    "        ExperimentResult(name=\"Validity Changes\", data_type=\"table\", data=valditity_changes),\n",
    "    ]\n",
    "\n",
    "    if experiment_logger_configs:\n",
    "        logger = ExperimentLoggerWanb(experiment_logger_configs)\n",
    "        dir_path = logger.store_results(experiment_results)\n",
    "        print(f\"Results stored at {dir_path}\")\n",
    "    return experiment_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def adversarial_experiment(\n",
    "    pred_training_module: BaseTrainingModule,\n",
    "    cf_module: BaseCFModule,\n",
    "    default_data_config: Dict[str, Any],\n",
    "    data_dir_list: List[str],\n",
    "    t_config: Dict[str, Any],\n",
    "    use_prev_model_params: bool = False,\n",
    "    return_best_model: bool = False, # return last model by default\n",
    "    experiment_logger_configs: Optional[ExperimentLoggerWanbConfigs] = None,\n",
    "    fast_dm: bool = False,\n",
    "):\n",
    "    hparams = deepcopy(default_data_config)\n",
    "    hparams.update(t_config)\n",
    "    if pred_training_module:\n",
    "        hparams.update(pred_training_module.hparams)\n",
    "    if experiment_logger_configs:\n",
    "        experiment_logger_configs.hparams = hparams\n",
    "\n",
    "    # data encoding\n",
    "    print(\"aggregating data...\")\n",
    "    default_data_config = _aggregate_default_data_encoders(default_data_config, data_dir_list)\n",
    "\n",
    "    # training models\n",
    "    # TODO: something is wrong here\n",
    "    if pred_training_module is not None:\n",
    "        print(\"start training...\")\n",
    "        model_params_opt_list, data_module_list = train_models(\n",
    "            pred_training_module, default_data_config,\n",
    "            data_dir_list, t_config, return_data_module_list=True,\n",
    "            use_fast=fast_dm\n",
    "        )\n",
    "    if isinstance(cf_module, BaseParametricCFModule):\n",
    "        print(\"start training CF Module...\")\n",
    "        model_params_opt_list, data_module_list = train_models(\n",
    "            cf_module.module, default_data_config,\n",
    "            data_dir_list, t_config, return_data_module_list=True\n",
    "        )\n",
    "\n",
    "    # else:\n",
    "    #     data_module_list = get_datamodules(default_data_config, data_dir_list)\n",
    "    #     model_params_opt_list = [(None, None)] * len(data_module_list)\n",
    "\n",
    "    # evaluate cfs\n",
    "    print(\"generating cfs...\")\n",
    "    experiment_results: List[ExperimentResult] = []\n",
    "    cf_results_list = []\n",
    "\n",
    "    # generate_cf_results_fn\n",
    "    # def generate_cf_results_fn(cf_module, dm, params, rng_key):\n",
    "    #     _params = deepcopy(params)\n",
    "    #     pred_fn = lambda x: pred_training_module.forward(_params, rng_key, x)\n",
    "    #     return generate_cf_results_local_exp(cf_module, dm, pred_fn=pred_fn) if is_local_cf_module \\\n",
    "    #         else generate_cf_results_cfnet(cf_module, dm, params=_params, rng_key=rng_key)\n",
    "    for i, ((params, _), dm) in enumerate(zip(model_params_opt_list, data_module_list)):\n",
    "        # cf_results = generate_cf_results_fn(\n",
    "        #     cf_module, dm,\n",
    "        #     params=params, rng_key=random.PRNGKey(0)\n",
    "        # )\n",
    "        if pred_training_module: \n",
    "            pred_fn = lambda x, params, rng: pred_training_module.forward(params, rng, x)\n",
    "        else: pred_fn = None\n",
    "        _params = deepcopy(params)\n",
    "        del params\n",
    "        \n",
    "        if isinstance(cf_module, BaseParametricCFModule):\n",
    "            cf_module.params = _params\n",
    "        \n",
    "        cf_exp = generate_cf_explanations(\n",
    "            cf_module, dm,\n",
    "            pred_fn=pred_fn,\n",
    "            t_configs=t_config,\n",
    "            pred_fn_args={'params': _params, 'rng': random.PRNGKey(0)},\n",
    "        )\n",
    "        cf_exp.cf_name = f\"model_{i}\"\n",
    "        cf_exp.dataset_name = f\"data_{i}\"\n",
    "        if isinstance(cf_module, BasePredFnCFModule):\n",
    "            pred_fn = lambda x, params, rng: cf_module.module.predict(params, rng, x)\n",
    "            cf_exp.pred_fn = _AuxPredFn(pred_fn, {'params': _params, 'rng': random.PRNGKey(0)})\n",
    "        cf_results_list.append(cf_exp)\n",
    "\n",
    "    experiment_results = _evaluate_adversarial_model(\n",
    "        cf_results_list, experiment_logger_configs=experiment_logger_configs)\n",
    "    return cf_results_list, experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# def adversarial_experiment_cfnet(\n",
    "#     training_module: CounterNetTrainingModule,\n",
    "#     default_data_config: Dict[str, Any],\n",
    "#     data_dir_list: List[str],\n",
    "#     t_config: Dict[str, Any],\n",
    "#     use_prev_model_params: bool = False,\n",
    "#     return_best_model: bool = False, # return last model by default\n",
    "#     experiment_logger_configs: Optional[ExperimentLoggerWanbConfigs] = None\n",
    "# ):\n",
    "#     return adversarial_experiment(\n",
    "#         pred_training_module=training_module,\n",
    "#         cf_module=training_module,\n",
    "#         default_data_config=default_data_config,\n",
    "#         data_dir_list=data_dir_list,\n",
    "#         t_config=t_config,\n",
    "#         is_local_cf_module=False,\n",
    "#         use_prev_model_params=use_prev_model_params,\n",
    "#         return_best_model=return_best_model,\n",
    "#         experiment_logger_configs=experiment_logger_configs\n",
    "#     )\n",
    "\n",
    "# def adversarial_experiment_local_exp(\n",
    "#     pred_training_module: CounterNetTrainingModule,\n",
    "#     cf_moudle: LocalCFExplanationModule,\n",
    "#     default_data_config: Dict[str, Any],\n",
    "#     data_dir_list: List[str],\n",
    "#     t_config: Dict[str, Any],\n",
    "#     use_prev_model_params: bool = False,\n",
    "#     return_best_model: bool = False, # return last model by default\n",
    "#     experiment_logger_configs: Optional[ExperimentLoggerWanbConfigs] = None\n",
    "# ):\n",
    "#     return adversarial_experiment(\n",
    "#         pred_training_module=pred_training_module,\n",
    "#         cf_module=cf_moudle,\n",
    "#         default_data_config=default_data_config,\n",
    "#         data_dir_list=data_dir_list,\n",
    "#         t_config=t_config,\n",
    "#         is_local_cf_module=True,\n",
    "#         use_prev_model_params=use_prev_model_params,\n",
    "#         return_best_model=return_best_model,\n",
    "#         experiment_logger_configs=experiment_logger_configs\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "cf_results_list = [\n",
    "    Explanation(\n",
    "        cf_name='m1',\n",
    "        dataset_name='d1',\n",
    "        X=jnp.array([\n",
    "            [1, 0, 1],\n",
    "            [1, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 1],\n",
    "        ]), # y_pred = [1, 1, 0, 0]\n",
    "        y=jnp.ones((4,1)),\n",
    "        cfs=jnp.array([\n",
    "            [0, 0, 1],\n",
    "            [0, 1, 0],\n",
    "            [1, 1, 0],\n",
    "            [1, 1, 1]\n",
    "        ]),\n",
    "        pred_fn=lambda x: x[:, 0], \n",
    "        total_time=0.1,\n",
    "        data_module=None\n",
    "    ),\n",
    "    Explanation(\n",
    "        cf_name='m2',\n",
    "        dataset_name='d2',\n",
    "        X=jnp.array([\n",
    "            [1, 0, 1],\n",
    "            [1, 1, 0],\n",
    "            [0, 1, 1],\n",
    "            [1, 0, 0]\n",
    "        ]), # y_pred = [1, 0, 1, 0]\n",
    "        y=jnp.ones((4,1)),\n",
    "        cfs=jnp.array([\n",
    "            [1, 0, 0],\n",
    "            [1, 1, 1],\n",
    "            [0, 1, 0],\n",
    "            [1, 0, 1]\n",
    "        ]),\n",
    "        pred_fn=lambda x: x[:, -1],\n",
    "        total_time=0.1,\n",
    "        data_module=None\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert compute_rob_validity(cf_results_list[0], cf_results_list[0].pred_fn) == 1.0\n",
    "assert compute_rob_validity(cf_results_list[0], cf_results_list[1].pred_fn) == 0.5\n",
    "assert compute_rob_validity(cf_results_list[1], cf_results_list[0].pred_fn) == 0.75\n",
    "assert compute_rob_validity(cf_results_list[1], cf_results_list[1].pred_fn) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>m1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     d1    d2\n",
       "m1  1.0  0.75\n",
       "m2  0.5  1.00"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_validity_matrix(cf_results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_configs = {\n",
    "    \"enc_sizes\": [50,10],\n",
    "    \"dec_sizes\": [10],\n",
    "    \"exp_sizes\": [50, 50],\n",
    "    \"dropout_rate\": 0.3,\n",
    "    'lr': 0.003,\n",
    "    \"lambda_1\": 1.0,\n",
    "    \"lambda_3\": 0.1,\n",
    "    \"lambda_2\": 0.2,\n",
    "    'adv_lr': 0.03\n",
    "}\n",
    "t_configs = {\n",
    "    'n_epochs': 2,\n",
    "    'monitor_metrics': 'val/val_loss',\n",
    "    'batch_size': 128,\n",
    "}\n",
    "data_configs = {\n",
    "    \"data_dir\": \"../assets/data/loan/year=2008.csv\",\n",
    "    \"data_name\": \"loan\",\n",
    "    'sample_frac': 0.1,\n",
    "    'batch_size': 128,\n",
    "    \"continous_cols\": [\n",
    "        \"NoEmp\", \"NewExist\", \"CreateJob\", \"RetainedJob\", \"DisbursementGross\", \"GrAppv\", \"SBA_Appv\"\n",
    "    ],\n",
    "    \"discret_cols\": [\n",
    "        \"State\", \"Term\", \"UrbanRural\", \"LowDoc\", \"Sector_Points\"\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RocourseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.module import PredictiveTrainingModule\n",
    "from rocourse_net.module import RoCourseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregating data...\n",
      "total data length: 89366\n",
      "preprocessing continuous features...\n",
      "preprocessing discret features...\n",
      "start training CF Module...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 30/30 [00:00<00:00, 49.78batch/s, train/adv_loss=0.06253081, train/train_loss_1=0.0621, train/train_loss_2=0.0625, train/train_loss_3=0.0391] \n",
      "Epoch 1: 100%|██████████| 17/17 [00:00<00:00, 51.66batch/s, train/adv_loss=nan, train/train_loss_1=0.143, train/train_loss_2=nan, train/train_loss_3=nan]            \n",
      "Epoch 1: 100%|██████████| 7/7 [00:00<00:00, 51.30batch/s, train/adv_loss=0.14924568, train/train_loss_1=0.108, train/train_loss_2=0.149, train/train_loss_3=0.0417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating cfs...\n",
      "calculating the validity matrix...\n"
     ]
    }
   ],
   "source": [
    "experiment_logger_configs = ExperimentLoggerWanbConfigs(\n",
    "    project_name='debug',\n",
    "    user_name='birkhoffg',\n",
    "    experiment_name='rocoursenet',\n",
    "    \n",
    ")\n",
    "cf_results_list, experiment_results = adversarial_experiment(\n",
    "    pred_training_module=None, #PredictiveTrainingModule({'lr': 0.003, 'sizes': [200, 10]}),\n",
    "    cf_module=RoCourseNet(m_configs),\n",
    "    default_data_config=data_configs,\n",
    "    data_dir_list=[ \n",
    "        f\"assets/data/loan/year={year}.csv\" for year in range(2007, 2010) \n",
    "    ],\n",
    "    # data_dir_list=[ \n",
    "    #     f\"assets/data/loan/year={year}.csv\" for year in range(1994, 2010) \n",
    "    # ],\n",
    "    t_config=t_configs,\n",
    "    # experiment_logger_configs=experiment_logger_configs\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VanillaCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.methods import VanillaCF\n",
    "from relax.module import PredictiveTrainingModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregating data...\n",
      "total data length: 89366\n",
      "preprocessing continuous features...\n",
      "preprocessing discret features...\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 30/30 [00:00<00:00, 91.07batch/s, train/train_loss_1=0.0644]\n",
      "Epoch 1: 100%|██████████| 17/17 [00:00<00:00, 90.14batch/s, train/train_loss_1=0.15]  \n",
      "Epoch 1: 100%|██████████| 7/7 [00:00<00:00, 94.06batch/s, train/train_loss_1=0.0994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating cfs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:22<00:00, 44.12it/s]\n",
      "100%|██████████| 1000/1000 [00:12<00:00, 83.25it/s]\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 182.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating the validity matrix...\n"
     ]
    }
   ],
   "source": [
    "cf_results_list, experiment_results = adversarial_experiment(\n",
    "    pred_training_module=PredictiveTrainingModule({'lr': 0.003, 'sizes': [200, 10]}),\n",
    "    cf_module=VanillaCF(),\n",
    "    default_data_config=data_configs,\n",
    "    data_dir_list=[ \n",
    "        f\"assets/data/loan/year={year}.csv\" for year in range(2007, 2010) \n",
    "    ],\n",
    "    # data_dir_list=[ \n",
    "    #     f\"assets/data/loan/year={year}.csv\" for year in range(1994, 2010) \n",
    "    # ],\n",
    "    t_config=t_configs,\n",
    "    # experiment_logger_configs=experiment_logger_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.3246443 , -0.66034925, -0.09777143, ...,  0.        ,\n",
       "               0.        ,  1.        ],\n",
       "             [-0.5055199 , -0.4706281 ,  0.07288305, ...,  0.        ,\n",
       "               0.        ,  0.        ],\n",
       "             [ 0.26521772, -0.57555026, -0.05948754, ...,  0.        ,\n",
       "               0.        ,  1.        ],\n",
       "             ...,\n",
       "             [ 0.3639497 , -0.70889324, -0.12165295, ...,  0.        ,\n",
       "               0.        ,  1.        ],\n",
       "             [ 0.4209727 , -0.80577576, -0.15645298, ...,  0.        ,\n",
       "               0.        ,  1.        ],\n",
       "             [ 0.25610036, -0.54156506, -0.06003434, ...,  0.        ,\n",
       "               0.        ,  1.        ]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_results_list[0].cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(12466, dtype=int32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_0 = cf_results_list[0].pred_fn(cf_results_list[0].X).reshape(-1, 1).round()\n",
    "y_pred_1 = cf_results_list[1].pred_fn(cf_results_list[0].X).reshape(-1, 1).round()\n",
    "(y_pred_0 == y_pred_1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12757"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': DeviceArray([-0.05756189, -0.23633571,  0.01710565, -0.05331689,\n",
       "              -0.3896599 , -0.27063757, -0.32480994, -0.14294973,\n",
       "              -0.04665818, -0.03992986,  0.03972355,  0.08615527,\n",
       "              -0.0744281 , -0.13791753, -0.13287549, -0.15250969,\n",
       "               0.564096  ,  0.08776585, -0.16287263, -0.13107836,\n",
       "              -0.09720206, -0.07217125, -0.06066768, -0.11944605,\n",
       "              -0.07659575, -0.08929113, -0.07949819, -0.09658174,\n",
       "              -0.09656797, -0.03666689, -0.05324696, -0.1618365 ,\n",
       "              -0.12976535,  0.62580806, -0.08158124, -0.03045881,\n",
       "              -0.09196167, -0.12518756, -0.08453243, -0.0775774 ,\n",
       "              -0.12637912, -0.09183856, -0.0062271 , -0.13629813,\n",
       "              -0.1257055 ,  0.11852361, -0.02613323, -0.11727765,\n",
       "              -0.15367144,  0.00479156, -0.06008328, -0.06138969,\n",
       "              -0.09986464,  0.25155354, -0.06922829, -0.13039733,\n",
       "              -0.11837475, -0.11905003, -0.00613969, -0.13261838,\n",
       "              -0.06677341,  0.03240969,  0.1871652 , -0.16410024,\n",
       "              -0.1692224 ,  0.43105456, -0.10232465, -0.28929108,\n",
       "              -0.02739567, -0.26596066, -0.27056018, -0.23834747,\n",
       "              -0.2807787 , -0.25139967, -0.21276985, -0.3191514 ,\n",
       "              -0.2448282 , -0.31101635, -0.03474026, -0.3191443 ,\n",
       "              -0.27490562, -0.19891305, -0.32483667, -0.2201054 ,\n",
       "              -0.30752945, -0.28376424,  0.00455241,  0.05898048,\n",
       "              -0.08037372,  0.05532676, -0.19013365,  0.18702184,\n",
       "              -0.10770813, -0.15214646, -0.1823272 , -0.03644034,\n",
       "               0.04687271, -0.00453571, -0.10655327,  0.13459715],            dtype=float32),\n",
       " 'w': DeviceArray([[ 0.07252895, -0.02071455, -0.05125423, ..., -0.05767903,\n",
       "               -0.11836613,  0.19947338],\n",
       "              [-0.00690783, -0.00729663,  0.10663713, ...,  0.19360219,\n",
       "                0.1276706 , -0.00159746],\n",
       "              [ 0.01176639, -0.0318793 ,  0.01708153, ..., -0.06147878,\n",
       "                0.04208125,  0.2988196 ],\n",
       "              ...,\n",
       "              [ 0.02184021, -0.04219056, -0.06379221, ..., -0.05363818,\n",
       "               -0.1280778 ,  0.10886073],\n",
       "              [ 0.02570571, -0.04339175, -0.03402676, ..., -0.04747041,\n",
       "               -0.10199858,  0.60901344],\n",
       "              [-0.06253002,  0.16261028, -0.08219639, ..., -0.25109494,\n",
       "                0.15531407,  0.01535862]], dtype=float32)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_results_list[0].pred_fn.fn_args['params']['counter_net_model/Explainer_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': DeviceArray([-0.05756189, -0.23633571,  0.01710565, -0.05331689,\n",
       "              -0.3896599 , -0.27063757, -0.32480994, -0.14294973,\n",
       "              -0.04665818, -0.03992986,  0.03972355,  0.08615527,\n",
       "              -0.0744281 , -0.13791753, -0.13287549, -0.15250969,\n",
       "               0.564096  ,  0.08776585, -0.16287263, -0.13107836,\n",
       "              -0.09720206, -0.07217125, -0.06066768, -0.11944605,\n",
       "              -0.07659575, -0.08929113, -0.07949819, -0.09658174,\n",
       "              -0.09656797, -0.03666689, -0.05324696, -0.1618365 ,\n",
       "              -0.12976535,  0.62580806, -0.08158124, -0.03045881,\n",
       "              -0.09196167, -0.12518756, -0.08453243, -0.0775774 ,\n",
       "              -0.12637912, -0.09183856, -0.0062271 , -0.13629813,\n",
       "              -0.1257055 ,  0.11852361, -0.02613323, -0.11727765,\n",
       "              -0.15367144,  0.00479156, -0.06008328, -0.06138969,\n",
       "              -0.09986464,  0.25155354, -0.06922829, -0.13039733,\n",
       "              -0.11837475, -0.11905003, -0.00613969, -0.13261838,\n",
       "              -0.06677341,  0.03240969,  0.1871652 , -0.16410024,\n",
       "              -0.1692224 ,  0.43105456, -0.10232465, -0.28929108,\n",
       "              -0.02739567, -0.26596066, -0.27056018, -0.23834747,\n",
       "              -0.2807787 , -0.25139967, -0.21276985, -0.3191514 ,\n",
       "              -0.2448282 , -0.31101635, -0.03474026, -0.3191443 ,\n",
       "              -0.27490562, -0.19891305, -0.32483667, -0.2201054 ,\n",
       "              -0.30752945, -0.28376424,  0.00455241,  0.05898048,\n",
       "              -0.08037372,  0.05532676, -0.19013365,  0.18702184,\n",
       "              -0.10770813, -0.15214646, -0.1823272 , -0.03644034,\n",
       "               0.04687271, -0.00453571, -0.10655327,  0.13459715],            dtype=float32),\n",
       " 'w': DeviceArray([[ 0.07252895, -0.02071455, -0.05125423, ..., -0.05767903,\n",
       "               -0.11836613,  0.19947338],\n",
       "              [-0.00690783, -0.00729663,  0.10663713, ...,  0.19360219,\n",
       "                0.1276706 , -0.00159746],\n",
       "              [ 0.01176639, -0.0318793 ,  0.01708153, ..., -0.06147878,\n",
       "                0.04208125,  0.2988196 ],\n",
       "              ...,\n",
       "              [ 0.02184021, -0.04219056, -0.06379221, ..., -0.05363818,\n",
       "               -0.1280778 ,  0.10886073],\n",
       "              [ 0.02570571, -0.04339175, -0.03402676, ..., -0.04747041,\n",
       "               -0.10199858,  0.60901344],\n",
       "              [-0.06253002,  0.16261028, -0.08219639, ..., -0.25109494,\n",
       "                0.15531407,  0.01535862]], dtype=float32)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_results_list[0].pred_fn.fn_args['params']['counter_net_model/Explainer_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': DeviceArray([-0.02574765,  0.09567051, -0.01645298,  0.02813452,\n",
       "               0.05758457,  0.06874906,  0.04412473,  0.1518215 ,\n",
       "              -0.03008602, -0.04655835, -0.03157707, -0.02536911,\n",
       "              -0.02293781, -0.06237506, -0.04882557, -0.0657744 ,\n",
       "               0.04763514,  0.08136597, -0.04336939, -0.03505868,\n",
       "              -0.05165615, -0.0383948 ,  0.00507054, -0.03148966,\n",
       "              -0.01416376,  0.0120643 ,  0.24699654, -0.06389464,\n",
       "              -0.00368025, -0.01580356,  0.01647298, -0.03387755,\n",
       "              -0.06293271, -0.076146  ,  0.01791069,  0.2717298 ,\n",
       "               0.04025881, -0.04208563, -0.05477343, -0.00336144,\n",
       "              -0.06703535, -0.02773977,  0.1618863 , -0.02318909,\n",
       "              -0.04799687,  0.0113631 , -0.00982577, -0.0662851 ,\n",
       "              -0.04215523, -0.03200383,  0.00206313, -0.02934637,\n",
       "              -0.09422823, -0.03384609, -0.03540881, -0.02038316,\n",
       "              -0.03813815,  0.01359431, -0.10401475, -0.10064937,\n",
       "              -0.11238529, -0.04443993, -0.02393051, -0.0808559 ,\n",
       "              -0.09190219,  0.33336252, -0.01645161, -0.15736775,\n",
       "               0.08434416, -0.19512849, -0.21245612, -0.17859314,\n",
       "              -0.1980802 , -0.08849739, -0.21181294, -0.17803884,\n",
       "              -0.13620834, -0.17260875,  0.06999055, -0.17656122,\n",
       "              -0.13810697, -0.08552403, -0.16495547, -0.08962827,\n",
       "              -0.23562922, -0.18783632, -0.06608594,  0.01976717,\n",
       "               0.02787918, -0.06702842, -0.14235927,  0.18935944,\n",
       "              -0.08473099, -0.13002598, -0.10249892, -0.04882869,\n",
       "               0.0212903 , -0.0098628 , -0.02780997,  0.09195969],            dtype=float32),\n",
       " 'w': DeviceArray([[ 0.08878098,  0.0113359 , -0.08272915, ..., -0.0517906 ,\n",
       "               -0.14618333,  0.16938266],\n",
       "              [ 0.0680909 , -0.24696435,  0.14154363, ...,  0.12627776,\n",
       "                0.02403161, -0.02361569],\n",
       "              [-0.03816092, -0.00694459,  0.10804107, ..., -0.02818616,\n",
       "                0.12858742,  0.15175176],\n",
       "              ...,\n",
       "              [-0.00499161, -0.0864815 , -0.09370095, ..., -0.05083577,\n",
       "               -0.10029151, -0.00108323],\n",
       "              [ 0.01422421, -0.07025536, -0.13105316, ..., -0.02500307,\n",
       "               -0.03678538,  0.42984518],\n",
       "              [-0.16152845,  0.1992481 , -0.00990648, ..., -0.25235972,\n",
       "                0.20135471,  0.02340216]], dtype=float32)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_results_list[1].pred_fn.fn_args['params']['counter_net_model/Explainer_1']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev2",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
