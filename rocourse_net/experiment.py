# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_experiment.ipynb.

# %% auto 0
__all__ = ['DATA_TYPE_TO_VIS_FN', 'compute_rob_validity', 'calculate_validity_matrix', 'get_datamodules', 'train_models',
           'wandb_vis_table', 'wandb_vis_heatmap', 'ExperimentLogger', 'ExperimentLoggerWanbConfigs',
           'ExperimentLoggerWanb', 'NormalizedProximity', 'adversarial_experiment']

# %% ../nbs/01_experiment.ipynb 2
from relax.import_essentials import *
from relax.data import TabularDataModule
from relax.trainer import train_model_with_states, TrainingConfigs
from relax.evaluate import (
    Explanation,
    accuracy,
    evaluate_cfs,
    benchmark_cfs,
    generate_cf_explanations,
    _AuxPredFn,
    BaseEvalMetrics,
    Validity,
    Proximity,
    PredictiveAccuracy
)
from relax.module import BaseTrainingModule
from relax.methods.base import BaseCFModule, BaseParametricCFModule, BasePredFnCFModule
from relax.utils import validate_configs, proximity

from .module import RoCourseNetTrainingModule
from copy import deepcopy
from functools import partial
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
import wandb
from pydantic import validator


# %% ../nbs/01_experiment.ipynb 4
def compute_rob_validity(cf_results: Explanation, shifted_pred_fn: Callable):
    pred_fn = cf_results.pred_fn
    y_pred = pred_fn(cf_results.X).reshape(-1, 1).round()
    y_prime = 1. - y_pred
    cf_y = shifted_pred_fn(cf_results.cfs).reshape(-1, 1).round()

    rob_validity = accuracy(y_prime, cf_y)
    return rob_validity.item()

# %% ../nbs/01_experiment.ipynb 6
def _aggregate_default_data_encoders(default_data_config: Dict[str, Any], data_dir_list: List[str]):
    # data encoding
    data = pd.concat(
        [pd.read_csv(data_dir) for data_dir in data_dir_list]
    )
    print(f"total data length: {len(data)}")
    if len(default_data_config['continous_cols']) != 0:
        print("preprocessing continuous features...")
        normalizer = MinMaxScaler().fit(
            data[default_data_config['continous_cols']]
        )
        default_data_config.update({"normalizer": normalizer})

    if len(default_data_config['discret_cols']) != 0:
        print("preprocessing discret features...")
        encoder = OneHotEncoder(sparse=False).fit(
            data[default_data_config['discret_cols']]
        )
        default_data_config.update({"encoder": encoder})
    return default_data_config

# %% ../nbs/01_experiment.ipynb 7
def calculate_validity_matrix(
    cf_results_list: Iterable[Explanation]
) -> pd.DataFrame:
    validity_matrix_dict = {}
    for i, cf_results_i in enumerate(cf_results_list):
        data_name = cf_results_i.dataset_name
        rob_validity = {}
        for j, cf_results_j in enumerate(cf_results_list):
            cf_name_j = cf_results_j.cf_name
            shifted_pred_fn = cf_results_j.pred_fn
            val = compute_rob_validity(
                cf_results_i, shifted_pred_fn
            )
            rob_validity[cf_name_j] = val
            # print(f'data_name: {data_name}; cf_name: {cf_name_j}; shifted_pred_fn: {shifted_pred_fn}; val: {val} ')
        validity_matrix_dict[data_name] = rob_validity
    return pd.DataFrame.from_dict(validity_matrix_dict)

# %% ../nbs/01_experiment.ipynb 9
def get_datamodules(
    default_data_configs: Dict[str, Any],
    data_dir_list: List[str]
):
    data_module_list = []
    for data_dir in data_dir_list:
        data_config = deepcopy(default_data_configs)
        data_config['data_dir'] = data_dir
        data_module = TabularDataModule(data_config)
        data_module_list.append(data_module)
    return data_module_list

# %% ../nbs/01_experiment.ipynb 10
def train_models(
    training_module: BaseTrainingModule,
    default_data_configs: Dict[str, Any],
    data_dir_list: List[str],
    t_configs: Dict[str, Any],
    return_data_module_list: bool = False
):
    model_params_opt_list = []
    data_module_list = []

    for i, data_dir in enumerate(data_dir_list):
        data_config = deepcopy(default_data_configs)
        data_config['data_dir'] = data_dir
        dm = TabularDataModule(data_config)
        params, opt_state = training_module.init_net_opt(
            dm, random.PRNGKey(42)
        )
        
        params, opt_state = train_model_with_states(
            training_module, params, opt_state, dm, t_configs
        )
        
        model_params_opt_list.append((params, opt_state))
        data_module_list.append(dm)
    if return_data_module_list:
        return model_params_opt_list, data_module_list
    else:
        return model_params_opt_list

# %% ../nbs/01_experiment.ipynb 12
def wandb_vis_table(table: pd.DataFrame):
    return wandb.Table(dataframe=table)

def wandb_vis_heatmap(heatmap: pd.DataFrame):
    return wandb.plots.HeatMap(x_labels=heatmap.columns, y_labels=heatmap.index, matrix_values=heatmap.values, show_text=False)

DATA_TYPE_TO_VIS_FN = {
    'table': wandb_vis_table,
    'heatmap': wandb_vis_heatmap
}

# %% ../nbs/01_experiment.ipynb 13
class ExperimentResult(BaseParser):
    name: str
    data_type: str
    data: Any

    @validator('data_type')
    def validate_data_type(cls, v):
        if v not in DATA_TYPE_TO_VIS_FN.keys():
            raise ValueError(f"`data_type` should be one of {DATA_TYPE_TO_VIS_FN.keys()}, but got {v}")
        return v

    def wandb_vis(self):
        return DATA_TYPE_TO_VIS_FN[self.data_type](self.data)

# %% ../nbs/01_experiment.ipynb 14
class ExperimentLogger(ABC):
    @abstractmethod
    def store_results(self, results: List[ExperimentResult]):
        raise NotImplementedError

# %% ../nbs/01_experiment.ipynb 15
class ExperimentLoggerWanbConfigs(BaseParser):
    project_name: str                           # `project`
    user_name: str                              # `entity`
    experiment_name: str                        # `name`
    hparams: Optional[Dict[str, Any]] = None    # hypterparamters


class ExperimentLoggerWanb(ExperimentLogger):
    def __init__(self, configs: ExperimentLoggerWanbConfigs):
        super().__init__()
        self.run = wandb.init(
            project=configs.project_name, entity=configs.user_name, name=configs.experiment_name, config=configs.hparams,
            settings=wandb.Settings(start_method="fork"))

    def store_results(self, results: List[ExperimentResult]):
        with self.run as run:
            run.log({
                r.name: r.wandb_vis() for r in results
            })
        return self.run.dir

# %% ../nbs/01_experiment.ipynb 17
class NormalizedProximity(BaseEvalMetrics):
    def __str__(self):
        return "NormalizedProximity"
    
    """Normalized proximity of counterfactuals to the original instance."""
    def __call__(self, cf_explanations: Explanation) -> float:
        X, _ = cf_explanations.data_module.test_dataset[:]
        return (proximity(X, cf_explanations.cfs) / X.shape[1]).item()

# %% ../nbs/01_experiment.ipynb 19
def calculate_validity_changes(val_matrix_df: pd.DataFrame):
    assert len(val_matrix_df.columns) == len(val_matrix_df.index), \
        f"val_matrix_df.columns={val_matrix_df.columns}, but val_matrix_df.index={val_matrix_df.index}"
    matrix = val_matrix_df.values
    n_datasets = len(matrix[0])

    validity = [matrix[i][i] for i in range(n_datasets)]
    w1_val_result, w1_dec_result = [], []
    for i in range(n_datasets - 1):
        w1_val_result.append(matrix[i][i+1])
        w1_dec_result.append(matrix[i][i] - matrix[i][i+1])

    wall_val_result, wall_dec_result =  [], []
    for i in range(n_datasets):
        for j in range(n_datasets):
            if j == i: continue
            wall_val_result.append(matrix[i][j])
            wall_dec_result.append(matrix[i][i] - matrix[i][j])
    result = {
        'cf_validity': { 'mean': np.average(validity), 'std': np.std(validity) },
        'cf_validity (w=1)': { 'mean': np.average(w1_val_result), 'std': np.std(w1_val_result) },
        'cf_validity (all)': { 'mean': np.average(wall_val_result), 'std': np.std(wall_val_result) },
        'validity_decrease (w=1)': { 'mean': np.average(w1_dec_result), 'std': np.std(w1_dec_result) },
        'validity_decrease (all)': { 'mean': np.average(wall_dec_result), 'std': np.std(wall_dec_result) }
    }
    return pd.DataFrame.from_dict(result)

# %% ../nbs/01_experiment.ipynb 20
def _evaluate_adversarial_model(
    cf_results_list: Iterable[Explanation],
    experiment_logger_configs: Optional[ExperimentLoggerWanbConfigs] = None,
):
    cf_results_df = benchmark_cfs(
        cf_results_list,
        # metrics=[
        #     PredictiveAccuracy(),
        #     Validity(),
        #     NormalizedProximity(),
        # ],
    )
    cf_aggre_df = (
        cf_results_df.describe()
        .loc[["mean", "std"]]
        .reset_index()
        .rename(columns={"index": "stat"})
    )

    print("calculating the validity matrix...")
    validity_matrix_df = calculate_validity_matrix(cf_results_list=cf_results_list)
    valditity_changes = calculate_validity_changes(validity_matrix_df)

    experiment_results = [
        ExperimentResult(name="CF Results", data_type="table", data=cf_results_df),
        ExperimentResult(name="CF Metrics", data_type="table", data=cf_aggre_df),
        ExperimentResult(name="Heatmap", data_type="heatmap", data=validity_matrix_df),
        ExperimentResult(name="Validity Matrix", data_type="table", data=validity_matrix_df),
        ExperimentResult(name="Validity Changes", data_type="table", data=valditity_changes),
    ]

    if experiment_logger_configs:
        logger = ExperimentLoggerWanb(experiment_logger_configs)
        dir_path = logger.store_results(experiment_results)
        print(f"Results stored at {dir_path}")
    return experiment_results


# %% ../nbs/01_experiment.ipynb 21
def adversarial_experiment(
    pred_training_module: BaseTrainingModule,
    cf_module: BaseCFModule,
    default_data_config: Dict[str, Any],
    data_dir_list: List[str],
    t_config: Dict[str, Any],
    use_prev_model_params: bool = False,
    return_best_model: bool = False, # return last model by default
    experiment_logger_configs: Optional[ExperimentLoggerWanbConfigs] = None
):
    hparams = deepcopy(default_data_config)
    hparams.update(t_config)
    if pred_training_module:
        hparams.update(pred_training_module.hparams)
    if experiment_logger_configs:
        experiment_logger_configs.hparams = hparams

    # data encoding
    print("aggregating data...")
    default_data_config = _aggregate_default_data_encoders(default_data_config, data_dir_list)

    # training models
    # TODO: something is wrong here
    if pred_training_module is not None:
        print("start training...")
        model_params_opt_list, data_module_list = train_models(
            pred_training_module, default_data_config,
            data_dir_list, t_config, return_data_module_list=True
        )
    if isinstance(cf_module, BaseParametricCFModule):
        print("start training CF Module...")
        model_params_opt_list, data_module_list = train_models(
            cf_module.module, default_data_config,
            data_dir_list, t_config, return_data_module_list=True
        )

    # else:
    #     data_module_list = get_datamodules(default_data_config, data_dir_list)
    #     model_params_opt_list = [(None, None)] * len(data_module_list)

    # evaluate cfs
    print("generating cfs...")
    experiment_results: List[ExperimentResult] = []
    cf_results_list = []

    # generate_cf_results_fn
    # def generate_cf_results_fn(cf_module, dm, params, rng_key):
    #     _params = deepcopy(params)
    #     pred_fn = lambda x: pred_training_module.forward(_params, rng_key, x)
    #     return generate_cf_results_local_exp(cf_module, dm, pred_fn=pred_fn) if is_local_cf_module \
    #         else generate_cf_results_cfnet(cf_module, dm, params=_params, rng_key=rng_key)
    for i, ((params, _), dm) in enumerate(zip(model_params_opt_list, data_module_list)):
        # cf_results = generate_cf_results_fn(
        #     cf_module, dm,
        #     params=params, rng_key=random.PRNGKey(0)
        # )
        if pred_training_module: 
            pred_fn = lambda x, params, rng: pred_training_module.forward(params, rng, x)
        else: pred_fn = None
        _params = deepcopy(params)
        del params
        
        if isinstance(cf_module, BaseParametricCFModule):
            cf_module.params = _params
        
        cf_exp = generate_cf_explanations(
            cf_module, dm,
            pred_fn=pred_fn,
            t_configs=t_config,
            pred_fn_args={'params': _params, 'rng': random.PRNGKey(0)},
        )
        cf_exp.cf_name = f"model_{i}"
        cf_exp.dataset_name = f"data_{i}"
        if isinstance(cf_module, BasePredFnCFModule):
            pred_fn = lambda x, params, rng: cf_module.module.predict(params, rng, x)
            cf_exp.pred_fn = _AuxPredFn(pred_fn, {'params': _params, 'rng': random.PRNGKey(0)})
        cf_results_list.append(cf_exp)

    experiment_results = _evaluate_adversarial_model(
        cf_results_list, experiment_logger_configs=experiment_logger_configs)
    return cf_results_list, experiment_results
